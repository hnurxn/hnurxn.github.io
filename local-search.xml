<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>tensorflow-gpu安装小tip</title>
    <link href="/2021/08/21/dl%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/"/>
    <url>/2021/08/21/dl%E5%90%84%E7%A7%8D%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/</url>
    
    <content type="html"><![CDATA[<h2 id="linux-kernel版本"><a class="markdownIt-Anchor" href="#linux-kernel版本"></a> Linux Kernel版本</h2><p>uname -a</p><h2 id="cuda版本"><a class="markdownIt-Anchor" href="#cuda版本"></a> CUDA版本</h2><p>CUDA 有两种API，分别是 <strong>运行时 API</strong> 和 <strong>驱动API</strong>，即所谓的 Runtime API 与 Driver API。<br /><strong>nvidia-smi</strong> 的结果除了有 GPU 驱动版本型号，还有 CUDA Driver API的型号，这里是 10.0。<br />而<strong>nvcc -V</strong>的结果是对应 CUDA Runtime API.</p><h2 id="cudnn版本"><a class="markdownIt-Anchor" href="#cudnn版本"></a> cudnn版本</h2><p>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</p><h2 id="版本对应关系"><a class="markdownIt-Anchor" href="#版本对应关系"></a> 版本对应关系</h2><p><strong>CUDA Toolkit and Minimum Required Driver Version for CUDA Enhanced Compatibility</strong></p><table><thead><tr><th>CUDA Toolkit</th><th style="text-align:center">Minimum Required Driver Version for CUDA Enhanced Compatibility</th></tr></thead><tbody><tr><td></td><td style="text-align:center">Linux x86_64 Driver Version</td></tr><tr><td>CUDA 11.4</td><td style="text-align:center">&gt;=450.80.02</td></tr><tr><td>CUDA 11.3</td><td style="text-align:center">&gt;=450.80.02</td></tr><tr><td>CUDA 11.2</td><td style="text-align:center">&gt;=450.80.02</td></tr><tr><td>CUDA 11.1 (11.1.0)</td><td style="text-align:center">&gt;=450.80.02</td></tr><tr><td>CUDA 11.0 (11.0.3)</td><td style="text-align:center">&gt;=450.36.06</td></tr></tbody></table><p><strong>CUDA Toolkit and Corresponding Driver Versions</strong></p><table><thead><tr><th>CUDA Toolkit</th><th>Toolkit Driver Version</th><th></th></tr></thead><tbody><tr><td></td><td>Linux x86_64 Driver Version</td><td>Windows x86_64 Driver Version</td></tr><tr><td>CUDA 11.4 Update 1</td><td>&gt;=470.57.02</td><td>&gt;=471.41</td></tr><tr><td>CUDA 11.4.0 GA</td><td>&gt;=470.42.01</td><td>&gt;=471.11</td></tr><tr><td>CUDA 11.3.1 Update 1</td><td>&gt;=465.19.01</td><td>&gt;=465.89</td></tr><tr><td>CUDA 11.3.0 GA</td><td>&gt;=465.19.01</td><td>&gt;=465.89</td></tr><tr><td>CUDA 11.2.2 Update 2</td><td>&gt;=460.32.03</td><td>&gt;=461.33</td></tr><tr><td>CUDA 11.2.1 Update 1</td><td>&gt;=460.32.03</td><td>&gt;=461.09</td></tr><tr><td>CUDA 11.2.0 GA</td><td>&gt;=460.27.03</td><td>&gt;=460.82</td></tr><tr><td>CUDA 11.1.1 Update 1</td><td>&gt;=455.32</td><td>&gt;=456.81</td></tr><tr><td>CUDA 11.1 GA</td><td>&gt;=455.23</td><td>&gt;=456.38</td></tr><tr><td>CUDA 11.0.3 Update 1</td><td>&gt;= 450.51.06</td><td>&gt;= 451.82</td></tr><tr><td>CUDA 11.0.2 GA</td><td>&gt;= 450.51.05</td><td>&gt;= 451.48</td></tr><tr><td>CUDA 11.0.1 RC</td><td>&gt;= 450.36.06</td><td>&gt;= 451.22</td></tr><tr><td>CUDA 10.2.89</td><td>&gt;= 440.33</td><td>&gt;= 441.22</td></tr><tr><td>CUDA 10.1 (10.1.105 general release, and updates)</td><td>&gt;= 418.39</td><td>&gt;= 418.96</td></tr><tr><td>CUDA 10.0.130</td><td>&gt;= 410.48</td><td>&gt;= 411.31</td></tr></tbody></table><h3 id="tensorflow-gpu"><a class="markdownIt-Anchor" href="#tensorflow-gpu"></a> Tensorflow-GPU</h3><table><thead><tr><th style="text-align:left">Version</th><th style="text-align:left">Python version</th><th style="text-align:left">Compiler</th><th style="text-align:left">Build tools</th><th style="text-align:left">cuDNN</th><th style="text-align:left">CUDA</th></tr></thead><tbody><tr><td style="text-align:left">tensorflow_gpu-2.6.0</td><td style="text-align:left">3.6-3.9</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 3.7.2</td><td style="text-align:left">8.1</td><td style="text-align:left">11.2</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.5.0</td><td style="text-align:left">3.6-3.9</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 3.7.2</td><td style="text-align:left">8.1</td><td style="text-align:left">11.2</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.4.0</td><td style="text-align:left">3.6-3.8</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 3.1.0</td><td style="text-align:left">8.0</td><td style="text-align:left">11.0</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.3.0</td><td style="text-align:left">3.5-3.8</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 3.1.0</td><td style="text-align:left">7.6</td><td style="text-align:left">10.1</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.2.0</td><td style="text-align:left">3.5-3.8</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 2.0.0</td><td style="text-align:left">7.6</td><td style="text-align:left">10.1</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.1.0</td><td style="text-align:left">3.5-3.7</td><td style="text-align:left">MSVC 2019</td><td style="text-align:left">Bazel 0.27.1-0.29.1</td><td style="text-align:left">7.6</td><td style="text-align:left">10.1</td></tr><tr><td style="text-align:left">tensorflow_gpu-2.0.0</td><td style="text-align:left">3.5-3.7</td><td style="text-align:left">MSVC 2017</td><td style="text-align:left">Bazel 0.26.1</td><td style="text-align:left">7.4</td><td style="text-align:left">10</td></tr></tbody></table><h2 id="安装"><a class="markdownIt-Anchor" href="#安装"></a> 安装</h2><p>1.首先创建新的环境env</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda create -n  env_name python == 3.x.x<br></code></pre></td></tr></table></figure><p>2.安装cudatoolkit和cudnn （自行检查版本）（本机公共环境已经装好的话，就可以不操作）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda install cudatoolkit=11.0<br>conda install cudnn=8.0 // 可以不写版本 将自动匹配版本<br></code></pre></td></tr></table></figure><p>这里的安装路径没有单独形成cuda文件夹，都是统一存放在envs/你的虚拟环境/lib or include 文件夹下.</p><p><strong>注意几个问题：</strong></p><blockquote><p>1.conda只有部分cudnn的版本，因此直接输入conda install cudnn 或者cudatookit 会自动安装对应版本的 cudatookit和cudnn</p><p>2.有一些具体的版本可能用pip和conda无法下载，换各种源如果仍不能下载，只能通过官网安装（网上教程都是按照到服务器公共资源上）</p><p>3.最简单的方法，新建环境后不做任何操作，直接conda install tensorflow-gpu=2.x.x，会显示提醒你需要附加哪些依赖，其中就包括cudatoolkit和cudnn，自己看清楚版本然后继续安装就行。</p></blockquote><p>3.查看conda当前可以按照的版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">conda search tensorflow-gpu<br></code></pre></td></tr></table></figure><p>4.安装tensorflow-gpu</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip  install tensorflow-gpu==2.4.1  <br>一些高版本 conda可能没有资源 需要用pip<br></code></pre></td></tr></table></figure><h4 id="最稳定的一个版本"><a class="markdownIt-Anchor" href="#最稳定的一个版本"></a> 最稳定的一个版本</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 完整步骤</span><br>conda install cudatoolkit=10.1<br>conda install cudnn=7.6.5<br>conda install tensorflow-gpu=2.2.0<br><span class="hljs-comment"># 或者直接进行下面一步：</span><br>conda install tensorflow-gpu=2.2.0<br><span class="hljs-comment">#检查提醒安装的依赖，cudnn和cudatoolkit版本没问题即可</span><br></code></pre></td></tr></table></figure><center><font color = blue size =6>概念扫盲</font></center><blockquote><p>参考https://www.cnblogs.com/marsggbo/p/11838823.html</p></blockquote><h3 id="gpu型号"><a class="markdownIt-Anchor" href="#gpu型号"></a> GPU型号</h3><ul><li><p><strong>显卡</strong>： 简单理解这个就是我们前面说的<strong>GPU</strong>，尤其指NVIDIA公司生产的GPU系列，因为后面介绍的cuda,cudnn都是NVIDIA公司针对自身的GPU独家设计的。</p></li><li><p><strong>显卡驱动</strong>：很明显就是字面意思，通常指<strong>NVIDIA Driver</strong>，其实它就是一个驱动软件，而前面的<strong>显卡</strong>就是硬件。</p></li><li><p>gpu架构：Tesla、Fermi、Kepler、Maxwell、Pascal</p></li><li><p>芯片型号：GT200、GK210、GM104、GF104等</p></li><li><p>显卡系列：GeForce、Quadro、Tesla</p></li><li><p>GeForce显卡型号：G/GS、GT、GTS、GTX</p><p>最后一个GeForce的显卡型号是不同的硬件定制，越往后性能越好，时钟频率越高显存越大，即G/GS&lt;GT&lt;GTS&lt;GTX</p></li></ul><h3 id="cuda"><a class="markdownIt-Anchor" href="#cuda"></a> CUDA</h3><p>CUDA英文全称是Compute Unified Device Architecture，是显卡厂商NVIDIA推出的运算平台。 CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。按照<a href="https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/">官方</a>的说法是，<strong>CUDA是一个并行计算平台和编程模型，能够使得使用GPU进行通用计算变得简单和优雅</strong>。</p><h3 id="cudnn"><a class="markdownIt-Anchor" href="#cudnn"></a> cudnn</h3><p>一个专门为深度学习计算设计的软件库，里面提供了很多专门的计算函数，如卷积等。</p><h3 id="cuda-toolkit"><a class="markdownIt-Anchor" href="#cuda-toolkit"></a> CUDA Toolkit</h3><p>CUDA Toolkit由以下组件组成：</p><ul><li><p><strong>Compiler</strong>: CUDA-C和CUDA-C++编译器<code>NVCC</code>位于<code>bin/</code>目录中。它建立在<code>NVVM</code>优化器之上，而<code>NVVM</code>优化器本身构建在<code>LLVM</code>编译器基础结构之上。希望开发人员可以使用<code>nvm/</code>目录下的Compiler SDK来直接针对NVVM进行开发。</p></li><li><p><strong>Tools</strong>: 提供一些像<code>profiler</code>,<code>debuggers</code>等工具，这些工具可以从<code>bin/</code>目录中获取</p></li><li><dl><dt><strong>Libraries</strong></dt><dd>下面列出的部分科学库和实用程序库可以在lib/目录中使用(Windows上的DLL位于bin/中)，它们的接口在include/目录中可获取。</dd></dl><ul><li><strong>cudart</strong>: CUDA Runtime</li><li><strong>cudadevrt</strong>: CUDA device runtime</li><li><strong>cupti</strong>: CUDA profiling tools interface</li><li><strong>nvml</strong>: NVIDIA management library</li><li><strong>nvrtc</strong>: CUDA runtime compilation</li><li><strong>cublas</strong>: BLAS (Basic Linear Algebra Subprograms，基础线性代数程序集)</li><li><strong>cublas_device</strong>: BLAS kernel interface</li><li>…</li></ul></li><li><p><strong>CUDA Samples</strong>: 演示如何使用各种CUDA和library API的代码示例。可在Linux和Mac上的<code>samples/</code>目录中获得，Windows上的路径是<code>C：\ProgramData\NVIDIA Corporation\CUDA Samples</code>中。在Linux和Mac上，<code>samples/</code>目录是只读的，如果要对它们进行修改，则必须将这些示例复制到另一个位置。</p></li><li><p><strong>CUDA Driver</strong>: 运行CUDA应用程序需要系统至少有一个<strong>具有CUDA功能的GPU</strong>和<strong>与CUDA工具包兼容的驱动程序</strong>。每个版本的CUDA工具包都对应一个最低版本的CUDA Driver，也就是说如果你安装的CUDA Driver版本比官方推荐的还低，那么很可能会无法正常运行。CUDA Driver是向后兼容的，这意味着根据CUDA的特定版本编译的应用程序将继续在后续发布的Driver上也能继续工作。通常为了方便，在安装CUDA Toolkit的时候会默认安装CUDA Driver。在开发阶段可以选择默认安装Driver，但是对于像Tesla GPU这样的商用情况时，建议在<a href="http://www.nvidia.com/drivers">官方</a>安装最新版本的Driver。</p></li></ul><h3 id="nvcc-nvidia-smi"><a class="markdownIt-Anchor" href="#nvcc-nvidia-smi"></a> nvcc &amp;nvidia-smi</h3><ul><li><code>nvcc</code>其实就是CUDA的编译器,可以从CUDA Toolkit的<code>/bin</code>目录中获取,类似于<code>gcc</code>就是c语言的编译器。由于程序是要经过编译器编程成可执行的二进制文件，而cuda程序有两种代码，一种是运行在cpu上的host代码，一种是运行在gpu上的device代码，所以<code>nvcc</code>编译器要保证两部分代码能够编译成二进制文件在不同的机器上执行。</li><li><code>nvidia-smi</code>全程是NVIDIA System Management Interface ，它是一个基于前面介绍过的<code>NVIDIA Management Library(NVML)</code>构建的命令行实用工具，旨在帮助管理和监控NVIDIA GPU设备</li></ul><p>二者显示的版本可能不同，stackoverflow有解释如下：</p><p>CUDA有两个主要的API：<strong>runtime(运行时) API</strong>和<strong>driver API</strong>。这两个API都有对应的CUDA版本（如9.2和10.0等）。</p><ul><li>用于支持<strong>driver API</strong>的必要文件(如<code>libcuda.so</code>)是由<strong>GPU driver installer</strong>安装的。<code>nvidia-smi</code>就属于这一类API。</li><li>用于支持<strong>runtime API</strong>的必要文件(如<code>libcudart.so</code>以及<code>nvcc</code>)是由<strong>CUDA Toolkit installer</strong>安装的。（CUDA Toolkit Installer有时可能会集成了GPU driver Installer）。<code>nvcc</code>是与CUDA Toolkit一起安装的CUDA compiler-driver tool，它只知道它自身构建时的CUDA runtime版本。它不知道安装了什么版本的GPU driver，甚至不知道是否安装了GPU driver。</li></ul><p>综上，如果driver API和runtime API的CUDA版本不一致可能是因为你使用的是单独的GPU driver installer，而不是CUDA Toolkit installer里的GPU driver installer。</p><h3 id="pip-conda"><a class="markdownIt-Anchor" href="#pip-conda"></a> pip &amp; conda</h3><p>Conda 和 pip 通常被认为几乎相同。 尽管这两个工具的某些功能重叠，但它们的<strong>设计目的是不同的</strong>。</p><p>Pip 是 Python Packaging Authority 推荐的用于从 Python Package Index（ PyPI） 安装包的工具。 Pip 安装打包为wheel或source distributions的 Python 软件。 后者可能要求系统在成功调用 pip 之前安装兼容的编译器和可能的库。</p><p>Conda 是一个跨平台的包和环境管理器，用于从 Anaconda 存储库和 Anaconda Cloud 安装和管理 conda 包。 Conda 包是二进制文件。 永远不需要编译器来安装它们。 此外，<strong>conda 包不仅限于 Python 软件。 它们还可能包含 C 或 C++ 库、R 包或任何其他软件</strong>。</p><p>conda 和 pip 之间的主要区别: <strong>Pip 安装 Python 包，而 conda 安装可能包含用任何语言编写的软件的包</strong>。 例如，在使用 pip 之前，必须通过系统包管理器或通过下载并运行安装程序来安装 Python 解释器。 另一方面，Conda 可以直接安装 Python 包以及 Python 解释器。</p><p>这两个工具之间的另一个主要区别是 <strong>conda 能够创建隔离的环境，其中可以包含不同版本的 Python 和安装在其中的包</strong>。 这在使用数据科学工具时非常有用，因为不同的工具可能包含相互冲突的需求，这可能会阻止它们全部安装到单个环境中。 Pip 没有对环境的内置支持，而是依赖于其他工具（如 virtualenv 或 venv）来创建隔离的环境。</p><p>Pip 和 conda 在如何实现环境中的依赖关系方面也有所不同。 安装软件包时，<strong>pip 会在递归的串行循环中安装依赖项</strong>。 没有努力确保所有包的依赖关系同时满足。 如果顺序中较早安装的软件包相对于顺序中较晚安装的软件包具有不兼容的依赖版本，这<strong>可能会导致环境以微妙的方式被破坏</strong>。 相比之下，conda 使用可满足性 (SAT) 求解器来验证是否满足环境中安装的所有软件包的所有要求。 此检查可能需要额外的时间，但有助于防止创建损坏的环境。 <strong>只要有关依赖项的包元数据是正确的，conda 就会按预期生成工作环境</strong>。</p><p>鉴于 conda 和 pip 之间的相似性，一些人尝试结合这些工具来创建数据科学环境也就不足为奇了。 将 pip 与 conda 结合使用的一个主要原因是<strong>当一个或多个软件包只能通过 pip 安装</strong>时。 Anaconda 存储库中提供了 1,500 多个包，包括最流行的数据科学、机器学习和 AI 框架。 这些，以及 Anaconda 云上数以千计的额外软件包，包括 conda-forge 和 bioconda，都可以使用 conda 进行安装。 尽管有这么多包，但与 PyPI 上提供的 150,000 多个包相比，它仍然很小。 有时需要一个包，它不能作为 conda 包提供，但<strong>在 PyPI 上可用并且可以使用 pip 安装</strong>。 在这些情况下，尝试<strong>同时使用 conda 和 pip 是有意义的</strong>。</p><table><thead><tr><th></th><th>conda</th><th>pip</th></tr></thead><tbody><tr><td>manages</td><td>binaries</td><td>wheel or source</td></tr><tr><td>can require compilers</td><td>no</td><td>yes</td></tr><tr><td>package types</td><td>any</td><td>Python-only</td></tr><tr><td>create environment</td><td>yes, built-in</td><td>no, requires virtualenv or venv</td></tr><tr><td>dependency checks</td><td>yes</td><td>no</td></tr><tr><td>package sources</td><td>Anaconda repo</td><td>cloud PyPI</td></tr></tbody></table>]]></content>
    
    
    <categories>
      
      <category>Tutorials</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>cuda</tag>
      
      <tag>tensorflow</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>风格迁移的pytorch实现</title>
    <link href="/2021/08/05/neural-style/"/>
    <url>/2021/08/05/neural-style/</url>
    
    <content type="html"><![CDATA[<h1 id="neural-style"><a class="markdownIt-Anchor" href="#neural-style"></a> neural-style</h1><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> introduction</h2><p>该论文提出了一种使用卷积神经网络将一幅图像的内容与另一幅图像的风格相结合的算法。</p><h2 id="install"><a class="markdownIt-Anchor" href="#install"></a> install</h2><p>安装<strong>torch7</strong>（基于Lua语言的深度学习框架）</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">略. 按github的<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">INSTALL</span>.</span></span>md没问题<br></code></pre></td></tr></table></figure><p>安装<strong>lua</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ apt install lua5.1<br>安装相关环境<br>$ sudo apt install liblua5.1-0<br>$ sudo apt-get install lua5.1-0-dev<br></code></pre></td></tr></table></figure><p>安装<strong>luarocks</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ wget http://luarocks.org/releases/luarocks-2.2.1.tar.gz<br>$ tar zxpf luarocks-2.2.1.tar.gz<br>$ <span class="hljs-built_in">cd</span> luarocks-2.2.1<br>$ ./configure<br>$ make build<br>$ make install<br></code></pre></td></tr></table></figure><p>安装<strong>loadcaffe</strong>（辅助包）</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">略. 按github的<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">INSTALL</span>.</span></span>md没问题<br></code></pre></td></tr></table></figure><h2 id="paper"><a class="markdownIt-Anchor" href="#paper"></a> paper</h2><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>相关数学知识</font></center></td></tr></table><p><font color = red size = 4>余弦相似度和内积</font>：</p><p><strong>余弦相似度</strong>：只考虑角度差   <strong>内积</strong>：考虑角度差和长度差</p><p><font color = red size =4>内积为什么能表示相似度</font>：</p><p><strong>内积可以反映出两个向量之间的某种关系或联系</strong>, 数字角度讲，当两个向量是<strong>归一化</strong>的情况下，如果两个值的差越大那么他们的乘积就越小，如果 1 * 3 &lt; 2 * 2</p><p><font color = red size=4>协方差矩阵和Gram矩阵</font>：</p><p><strong>协方差</strong>：度量各个维度偏离其均值的程度，衡量两个变量的总体误差，<font color = blue>协方差的值如果为正，则说明两者正相关</font>，方差是其中一个特例。如果正相关，每个样本对（Xi, Yi）,　即求和项大部分都是正数，即两个同方向偏离各自均值，而不同时偏离的也有，但是少，这样当样本多时，总和结果为正。</p><p><strong>协方差矩阵</strong>：协方差是不同随机变量之间的度量值，而协方差矩阵是推广到不同随机向量之间的度量。比如变量x和y →多元变量即向量X(x1,x2…,xk)自身各个变量之间作比较,协方差矩阵的每个元素是各个向量元素之间的协方差。<font color = blue>对角线元素(i,i)是数据第i维的方差，非对角线元素(i,j)是第i维和第j维的协方差</font>。</p><p><strong>Gram 矩阵</strong> :n维欧式空间中任意k个向量（feature向量）之间两两的内积所组成的矩阵。可以看做feature之间的<font color = blue>偏心协方差矩阵</font>（即没有减去均值的协方差矩阵）。比如在C × feature map（H×W）中，每个数字都来自于一个特定滤波器在特定位置的卷积，因此每个数字代表一个特征在特定位置的强度，而Gram计算的实际上是<font color = blue>两两特征之间的相关性，哪两个特征是同时出现的，哪两个是此消彼长</font>的等等，同时，Gram的对角线元素，还体现了每个特征在图像中出现的量，因此，Gram有助于把握整个图像的大体风格。</p><p>**Gram矩阵应用于风格迁移：**准备基准图像和风格图像  →  使用深层网络分别提取基准图像（加白噪声）和风格图像的特征向量（或者说是特征图feature map） →  分别计算两个图像的特征向量的Gram矩阵，以两个图像的Gram矩阵的差异最小化为优化目标，不断调整基准图像，使风格不断接近目标风格图像。 核心思想：提取的特征图中，<font color = blue>浅层网络提取的是局部的细节纹理特征，深层网络提取的是更抽象的轮廓、大小等信息</font>，这些特征总的结合起来表现出来的感觉就是图像的风格，由这些特征向量计算出来的的Gram矩阵，就可以<font color = blue>把图像特征之间隐藏的联系提取出来</font>，也就是各个特征之间的相关性高低。</p><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>论文主要内容</font></center></td></tr></table><p>作者在他之前的一篇文章提到（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1505.07376.pdf">Texture Synthesis Using Convolutional Neural Networks</a>）图像经过**卷积层后得到的特征图的协方差矩阵可以很好地表征图像的纹理特征，**但是会损失位置信息。不过在风格迁移的任务中，我们可以忽略位置信息损失这个缺点，只需要找到一个方法可以表征图像的纹理信息，并把它这些纹理信息迁移到需要被风格迁移的图像中，完成风格迁移的任务；而现在，利用协方差矩阵可以得到纹理信息，我们就可以完成风格迁移。</p><p>协方差是一个二阶的统计信息，文章里使用<strong>Gram matrix来代替协方差矩阵</strong>（其实就是没有减去均值的协方差矩阵），它能够描述全局特征的自相关。</p><p>Gram 矩阵的表达式：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>G</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>=</mo><munder><mo>∑</mo><mi>k</mi></munder><msubsup><mi>F</mi><mrow><mi>i</mi><mi>k</mi></mrow><mi>l</mi></msubsup><msubsup><mi>F</mi><mrow><mi>j</mi><mi>k</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">G_{ij}^l=\sum_k F_{ik}^l F_{jk}^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.282216em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.3521180000000004em;vertical-align:-1.3021129999999999em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0500050000000005em;"><span style="top:-1.847887em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021129999999999em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>F</mi><mrow><mi>i</mi><mi>k</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">F_{ik}^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.132216em;vertical-align:-0.2831079999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4168920000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>F</mi><mrow><mi>j</mi><mi>k</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">F_{jk}^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2683239999999998em;vertical-align:-0.4192159999999999em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4168920000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4192159999999999em;"><span></span></span></span></span></span></span></span></span></span>指的是 $ l $ 层特征图flatten之后的向量元素(比如第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span>层网络的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span>个特征向量的第<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span>个元素)，理解：假设图片经过某层卷积层后得到了[height, width, channel]的特征图，为了进行Gram matrix的操作，需要将特征图进行flatten，得到 F=[<strong>height*width</strong>, channel] 的特征图，那么这里的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>G</mi><mo>=</mo><mi>F</mi><mi mathvariant="normal">.</mi><mi>t</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>p</mi><mi>o</mi><mi>s</mi><mi>e</mi><mo>∗</mo><mi>F</mi></mrow><annotation encoding="application/x-tex">G=F.transpose* F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">G</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord">.</span><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">n</span><span class="mord mathdefault">s</span><span class="mord mathdefault">p</span><span class="mord mathdefault">o</span><span class="mord mathdefault">s</span><span class="mord mathdefault">e</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>, 得到size为[channel, channel]的G矩阵，表达特征图里面的自相关.</p><p>从直观上理解 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mi>m</mi></msub></mrow><annotation encoding="application/x-tex">F_m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">F_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>（ <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>m</mi><mo separator="true">,</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m,n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">m</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">n</span></span></span></span>是特征图 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>的列序号）记录了特征图的值，如果两列都是比较大的值（风格特征明显），那么相乘之后的响应也会变大（即风格得到了放大)；相反则是比较小的值，根本不需要管，于是我们的G就记录了图片的风格信息.</p><p><strong>损失函数</strong></p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><mi>α</mi><mi mathvariant="normal">∗</mi><msub><mi>L</mi><mrow><mi>s</mi><mi>t</mi><mi>y</mi><mi>l</mi><mi>e</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">)</mo><mo>+</mo><mi>β</mi><mi mathvariant="normal">∗</mi><msub><mi>L</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>a</mi><mo separator="true">,</mo><mi>f</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(a,f,p)=α∗L_{style}(p,f)+β∗L_{content}(a,f)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mord">∗</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mclose">)</span></span></span></span></span></p><p>很明显，Loss的组成就是两个部分：内容损失和风格损失。至于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span></span></span></span>都是可调，都是经验值。</p><p>主要分为两部分，保证内容图的正确，保证风格的迁移。如下所示：</p><p>内容损失：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>x</mi><mo separator="true">,</mo><mi>l</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msub><mi>F</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mi mathvariant="normal">−</mi><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L_{content}(p,x,l)=\frac12\sum_{ij}(F_{ij}−P_{ij})^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.735217em;vertical-align:-1.413777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord">2</span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord">1</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8723309999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">−</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">F_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>P</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>各自表示<strong>内容图</strong>经过某一层卷积层得到的特征图、<strong>生成图</strong>经过卷积层生成的特征图。把他们两个作差并平方求和（不仅记录一层卷积层特征图的差异)，使生成图的特征图越接近于内容图的特征图，这样就可以保留内容信息。一般来说，可选择比较浅的层数但别选第一层得到的特征图，因为这样会过于保留内容导致风格信息无法融入到特征图中。一般也只是选择两到三层作为内容损失函数。这里并没有用到Gram matrix.</p><p>风格损失：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mrow><mi>s</mi><mi>t</mi><mi>y</mi><mi>l</mi><mi>e</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mn>4</mn><msubsup><mi>N</mi><mi>l</mi><mn>2</mn></msubsup><msubsup><mi>M</mi><mi>l</mi><mn>2</mn></msubsup></mrow></mfrac><munder><mo>∑</mo><mrow><mi>i</mi><mi>j</mi></mrow></munder><mo stretchy="false">(</mo><msubsup><mi>G</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><mo>−</mo><msubsup><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L_{style}=\frac{1}{4N_{l}^2M_{l}^2}\sum_{ij}(G_{ij}^l-A_{ij}^l)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">y</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mord mathdefault mtight">e</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.735217em;vertical-align:-1.413777em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">4</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959079999999998em;"><span style="top:-2.398692em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30130799999999996em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7959079999999998em;"><span style="top:-2.398692em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span style="top:-3.0448em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30130799999999996em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9873080000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8723309999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.282216em;vertical-align:-0.383108em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.383108em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>G</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">G_{ij}^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2438799999999999em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathdefault">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msubsup><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">A_{ij}^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2438799999999999em;vertical-align:-0.394772em;"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.441336em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.394772em;"><span></span></span></span></span></span></span></span></span></span>代表生成图和风格图经过卷积层得到的特征图、然后自相关得到的Gram matrix。一样用平方误差作为损失函数来使两者接近.</p><p>整体框架：</p><pre><code class=" mermaid">graph LRA[目标图片]--&gt;|内容学习|BC[风格图片]--&gt;|风格学习|B[白噪声随机图片]B[白噪声随机图片]--&gt;|梯度下降优化|B[白噪声随机图片]B[白噪声随机图片]--&gt;|训练完成|D[迁移风格后的图片]</code></pre><p>细节：生成图的size和内容图的size一致（因为风格是要迁移到内容图上），生成图初始化是一个高斯白噪声的图片，它经过VGG网络后同样也生成了特征图，它既要与内容图生成的特征图计算内容损失，也要和风格图生成的Gram matrix计算风格损失，而每次前向传播得到的loss，终将反馈到它自身，然后它开始变化、接近于我们想要的结果，<strong>即这个风格迁移的网络其实并不需要训练网络结构内的任何权重参数，需要训练的是生成图里面的元素</strong>（在tensorflow里就是tf.Variable初始化的是生成图，而不是任何weight）。</p><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>代码复现</font></center></td></tr></table><p><strong>知识点补充</strong></p><p><strong>mata格式：</strong></p><p>模块scipy.io的函数loadmat和savemat可以实现Python对mat数据的读写。</p><p><strong>pytorch的计算图</strong>：</p><ul><li>机器学习的本质：给定<strong>包含多个参数的高维函数</strong><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>θ</mi><mn>1</mn><mo separator="true">,</mo><mi>θ</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(\theta1,\theta2,...)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mclose">)</span></span></span></span>,这个函数可以输入数据集，每输入一个或多个数据后，计算loss函数，利用梯度下降可以调整参数大小使loss值变小。</li><li>pytorch把这些计算保存到一个计算图里面，其实可以看作<strong>一颗树</strong>，进行前向传播</li><li>定义参数为<strong>Variable形式</strong>，并且<strong>requires_grad=True</strong>,因为参数是待修改的，需要进行梯度下降的，这样pytorch才能认识，参数便是所谓<strong>待优化的变量</strong>。</li><li>pytorch设计中，<strong>梯度默认会保存累加</strong>的，所以一般反向传播一次都会进行清空，也就是optimizer.zero_grad()</li></ul><p><strong>核心代码Pytorch</strong>：</p><p><strong>数据处理：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载图片  图片读取 -&gt; 神经网络可以读入的Tensor张量</span><br>loader = transforms.Compose([<br>    transforms.Resize([imsize,imsize]),  <span class="hljs-comment"># scale imported image</span><br>    transforms.ToTensor()])  <span class="hljs-comment"># transform it into a torch tensor</span><br> <br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">image_loader</span>(<span class="hljs-params">image_name</span>):</span><br>    image = Image.<span class="hljs-built_in">open</span>(image_name) <span class="hljs-comment">#图片路径</span><br>    <span class="hljs-comment"># fake batch dimension required to fit network&#x27;s input dimensions</span><br>    image = loader(image).unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-comment">#给前面加一个维度(1,..)</span><br>    <span class="hljs-keyword">return</span> image.to(device, torch.<span class="hljs-built_in">float</span>)<br>style_img = image_loader(<span class="hljs-string">&quot;examples/style.jpg&quot;</span>)<br>content_img = image_loader(<span class="hljs-string">&quot;examples/content.jpg&quot;</span>)<br><span class="hljs-comment"># 显示图片  Tensor张量维度变化 -&gt; 可显示的图片</span><br>unloader = transforms.ToPILImage()  <span class="hljs-comment"># reconvert into PIL image</span><br>plt.ion() <span class="hljs-comment">#打开交互模式</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">imshow</span>(<span class="hljs-params">tensor, title=<span class="hljs-literal">None</span></span>):</span><br>    image = tensor.cpu().clone()  <span class="hljs-comment"># we clone the tensor to not do changes on it</span><br>    image = image.squeeze(<span class="hljs-number">0</span>)      <span class="hljs-comment"># remove the fake batch dimension 去除第一个维度</span><br>    image = unloader(image)<br>    plt.imshow(image)<br>    <span class="hljs-keyword">if</span> title <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        plt.title(title)<br>    plt.pause(<span class="hljs-number">0.001</span>) <span class="hljs-comment"># pause a bit so that plots are updated</span><br><span class="hljs-comment"># 依次显示读入的图</span><br>plt.figure()<br>imshow(style_img, title=<span class="hljs-string">&#x27;Style Image&#x27;</span>)<br>plt.figure()<br>imshow(content_img, title=<span class="hljs-string">&#x27;Content Image&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>计算gram矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gram_matrix</span>(<span class="hljs-params"><span class="hljs-built_in">input</span></span>):</span><br>    a, b, c, d = <span class="hljs-built_in">input</span>.size()  <span class="hljs-comment"># a=batch size(=1)</span><br>    <span class="hljs-comment"># 特征映射 b=number</span><br>    <span class="hljs-comment"># (c,d)=dimensions of a f. map (N=c*d)</span><br> <br>    features = <span class="hljs-built_in">input</span>.view(a * b, c * d)  <span class="hljs-comment"># resise F_XL into \hat F_XL  a*b是特征的数量 c*b是一个特征向量的长度</span><br> <br>    G = torch.mm(features, features.t())  <span class="hljs-comment"># compute the gram product</span><br> <br>    <span class="hljs-comment"># 我们通过除以每个特征映射中的元素数来“标准化”gram矩阵的值.</span><br>    <span class="hljs-keyword">return</span> G.div(a * b * c * d)<br></code></pre></td></tr></table></figure><p><strong>计算loss函数</strong></p><p><font color= blue><strong>补充知识</strong> ：pytorch继承 nn.Module的方法</font></p><p>我们在定义自已的网络的时候，需要继承nn.Module类，并重新实现构造函数__init__构造函数和forward这两个方法。但有一些注意技巧：</p><ul><li>一般把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数__init__()中，当然我也可以吧不具有参数的层也放在里面；</li><li>一般把不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)可放在构造函数中，也可不放在构造函数中，如果不放在构造函数__init__里面，则在forward方法里面可以使用nn.functional来代替</li><li>forward方法是必须要重写的，它是实现模型的功能，实现各个层之间的连接关系的核心。</li></ul><p>所有放在构造函数__init__里面的层的都是这个模型的“固有属性”。</p><p>官方例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br> <br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Model</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span><br>        <span class="hljs-comment"># 固定内容</span><br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br> <br>        <span class="hljs-comment"># 定义相关的函数</span><br>        self.conv1 = nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>, <span class="hljs-number">5</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span><br>        <span class="hljs-comment"># 构建模型结构，可以使用F函数内容，其他调用__init__里面的函数</span><br>        x = F.relu(self.conv1(x))<br> <br>        <span class="hljs-comment"># 返回最终的结果</span><br>        <span class="hljs-keyword">return</span> F.relu(self.conv2(x))<br></code></pre></td></tr></table></figure><p><font color= blue><strong>补充知识</strong> ：pytorch的Sequential类使用方法</font></p><p>当一个模型较简单的时候，我们可以使用torch.nn.Sequential类来实现简单的顺序连接模型。这个模型也是继承自Module类的.</p><p><strong>torch的核心是Module类</strong>.实际上add_module()方法是定义在它的父类Module里面的，Sequential继承了而已.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br>model = nn.Sequential()<br>model.add_module(<span class="hljs-string">&quot;conv1&quot;</span>,nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">20</span>,<span class="hljs-number">5</span>))<br>model.add_module(<span class="hljs-string">&#x27;relu1&#x27;</span>, nn.ReLU())<br>model.add_module(<span class="hljs-string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="hljs-number">20</span>,<span class="hljs-number">64</span>,<span class="hljs-number">5</span>))<br>model.add_module(<span class="hljs-string">&#x27;relu2&#x27;</span>, nn.ReLU())<br> <br><span class="hljs-built_in">print</span>(model)<br><span class="hljs-built_in">print</span>(model[<span class="hljs-number">2</span>]) <span class="hljs-comment"># 通过索引获取第几个层</span><br><span class="hljs-string">&#x27;&#x27;&#x27;运行结果为：</span><br><span class="hljs-string">Sequential(</span><br><span class="hljs-string">  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (1): ReLU()</span><br><span class="hljs-string">  (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">  (3): ReLU()</span><br><span class="hljs-string">)</span><br><span class="hljs-string">Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>具体代码：</p><pre><code class=" mermaid">graph LR;A[input白噪音] --&gt; B[卷积层]B[卷积层] --&gt;C[池化层]C --&gt; D[...]D --&gt; E[content_loss或style_loss]E --&gt; e[卷积层]e[卷积层] --&gt;f[池化层]</code></pre><p>content_loss和style_loss层具体就是计算input和内容图的特征损失，以及input和风格图的特征gram矩阵损失，在构造该Sequential网络时，已经把内容图的特征和风格图特征的gram矩阵计算好了，之后每次输入input，直接计算损失值，然后通过梯度下降调整input的数值，再次输入input，重复进行。<strong>input就是该网络的可修改参数</strong>。<strong>训练多次后的input就是风格迁移的图片。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ContentLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, target,</span>):</span><br>        <span class="hljs-built_in">super</span>(ContentLoss, self).__init__()<br>        <span class="hljs-comment"># 我们从用于动态计算梯度的树中“分离”目标内容：</span><br>        <span class="hljs-comment"># 这是一个声明的值，而不是变量。 </span><br>        <span class="hljs-comment"># 否则标准的正向方法将引发错误。</span><br>        self.target = target.detach()<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):</span><br>        self.loss = F.mse_loss(<span class="hljs-built_in">input</span>, self.target)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span><br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">StyleLoss</span>(<span class="hljs-params">nn.Module</span>):</span><br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, target_feature</span>):</span><br>        <span class="hljs-built_in">super</span>(StyleLoss, self).__init__()<br>        self.target = gram_matrix(target_feature).detach()<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):</span><br>        G = gram_matrix(<span class="hljs-built_in">input</span>)<br>        self.loss = F.mse_loss(G, self.target)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span><br><br><span class="hljs-comment"># 创建一个模块来规范化输入图像</span><br><span class="hljs-comment"># 这样我们就可以轻松地将它放入nn.Sequential中 </span><br>cnn_normalization_mean = torch.tensor([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>]).to(device)<br>cnn_normalization_std = torch.tensor([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]).to(device)<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Normalization</span>(<span class="hljs-params">nn.Module</span>):</span><br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, mean, std</span>):</span><br>        <span class="hljs-built_in">super</span>(Normalization, self).__init__()<br>        <span class="hljs-comment"># .view the mean and std to make them [C x 1 x 1] so that they can</span><br>        <span class="hljs-comment"># directly work with image Tensor of shape [B x C x H x W].</span><br>        <span class="hljs-comment"># B is batch size. C is number of channels. H is height and W is width.</span><br>        self.mean = torch.tensor(mean).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        self.std = torch.tensor(std).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br> <br>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, img</span>):</span><br>        <span class="hljs-comment"># normalize img</span><br>        <span class="hljs-keyword">return</span> (img - self.mean) / self.std<br><span class="hljs-comment"># 加载vgg19模型，保留features部分，即不要全连接层</span><br>cnn = models.vgg19(pretrained=<span class="hljs-literal">True</span>).features.to(device).<span class="hljs-built_in">eval</span>()<br><br>content_layers_default = [<span class="hljs-string">&#x27;conv_4&#x27;</span>]<br>style_layers_default = [<span class="hljs-string">&#x27;conv_1&#x27;</span>, <span class="hljs-string">&#x27;conv_2&#x27;</span>, <span class="hljs-string">&#x27;conv_3&#x27;</span>, <span class="hljs-string">&#x27;conv_4&#x27;</span>, <span class="hljs-string">&#x27;conv_5&#x27;</span>]<br> <br><span class="hljs-comment"># 输入vgg网络（除去全连接层的部分）以及内容图和风格图，构造新的网络（加了一些计算损失函数的层，不涉及损失函数的层则可以直接删去）</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_style_model_and_losses</span>(<span class="hljs-params">cnn, normalization_mean, normalization_std,</span></span><br><span class="hljs-params"><span class="hljs-function">                               style_img, content_img,</span></span><br><span class="hljs-params"><span class="hljs-function">                               content_layers=content_layers_default,</span></span><br><span class="hljs-params"><span class="hljs-function">                               style_layers=style_layers_default</span>):</span><br>    cnn = copy.deepcopy(cnn)<br> <br>    <span class="hljs-comment"># 规范化模块</span><br>    normalization = Normalization(normalization_mean, normalization_std).to(device)<br> <br>    <span class="hljs-comment"># 只是为了拥有可迭代的访问权限或列出内容/系统损失</span><br>    content_losses = []<br>    style_losses = []<br> <br>    <span class="hljs-comment"># 假设cnn是一个`nn.Sequential`，</span><br>    <span class="hljs-comment"># 所以我们创建一个新的`nn.Sequential`来放入应该按顺序激活的模块</span><br>    model = nn.Sequential(normalization)<br> <br>    i = <span class="hljs-number">0</span>  <span class="hljs-comment"># increment every time we see a conv</span><br>    <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> cnn.children():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer, nn.Conv2d):<br>            i += <span class="hljs-number">1</span><br>            name = <span class="hljs-string">&#x27;conv_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, nn.ReLU):<br>            name = <span class="hljs-string">&#x27;relu_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)<br>            <span class="hljs-comment"># 对于我们在下面插入的`ContentLoss`和`StyleLoss`，</span><br>            <span class="hljs-comment"># 本地版本不能很好地发挥作用。所以我们在这里替换不合适的</span><br>            layer = nn.ReLU(inplace=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, nn.MaxPool2d):<br>            name = <span class="hljs-string">&#x27;pool_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(layer, nn.BatchNorm2d):<br>            name = <span class="hljs-string">&#x27;bn_&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(i)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Unrecognized layer: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(layer.__class__.__name__))<br> <br>        model.add_module(name, layer)<br> <br>        <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> content_layers:<br>            <span class="hljs-comment"># 加入内容损失:</span><br>            target = model(content_img).detach()<br>            content_loss = ContentLoss(target)<br>            model.add_module(<span class="hljs-string">&quot;content_loss_&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i), content_loss)<br>            content_losses.append(content_loss)<br> <br>        <span class="hljs-keyword">if</span> name <span class="hljs-keyword">in</span> style_layers:<br>            <span class="hljs-comment"># 加入风格损失:</span><br>            target_feature = model(style_img).detach()<br>            style_loss = StyleLoss(target_feature)<br>            model.add_module(<span class="hljs-string">&quot;style_loss_&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(i), style_loss)<br>            style_losses.append(style_loss)<br>    <span class="hljs-comment"># 现在我们在最后的内容和风格损失之后剪掉了图层</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(model) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(model[i], ContentLoss) <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(model[i], StyleLoss):<br>            <span class="hljs-keyword">break</span><br> <br>    model = model[:(i + <span class="hljs-number">1</span>)]<br> <br>    <span class="hljs-keyword">return</span> model, style_losses, content_losses<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_input_optimizer</span>(<span class="hljs-params">input_img</span>):</span><br>    <span class="hljs-comment"># 此行显示输入是需要渐变的参数</span><br>    optimizer = optim.LBFGS([input_img.requires_grad_()]) <span class="hljs-comment">#定义优化器，参数是输入input（一般网络的参数都是net.parameter()即权重和偏移）</span><br>    <span class="hljs-keyword">return</span> optimizer<br><span class="hljs-comment"># 实例化网络，并进行训练的全过程</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_style_transfer</span>(<span class="hljs-params">cnn, normalization_mean, normalization_std,</span></span><br><span class="hljs-params"><span class="hljs-function">                       content_img, style_img, input_img, num_steps=<span class="hljs-number">300</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">                       style_weight=<span class="hljs-number">10000</span>, content_weight=<span class="hljs-number">1</span></span>):</span><br>    <span class="hljs-string">&quot;&quot;&quot;Run the style transfer.&quot;&quot;&quot;</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Building the style transfer model..&#x27;</span>)<br>    <span class="hljs-comment">#构造网络完成  input作为输入，整个传递过程（高维函数）会计算损失函数的值</span><br>    model, style_losses, content_losses = get_style_model_and_losses(cnn,<br>        normalization_mean, normalization_std, style_img, content_img)<br>    <span class="hljs-comment">#定义优化器，input_img即（损失函数f(x,w,b)中的w,b 即待更新）</span><br>    optimizer = get_input_optimizer(input_img)<br> <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Optimizing..&#x27;</span>)<br>    run = [<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">while</span> run[<span class="hljs-number">0</span>] &lt;= num_steps:<br> <br>        <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">closure</span>():</span><br>            <span class="hljs-comment"># 更正更新的输入图像的值clamp限制数值在[0,1]范围内</span><br>            input_img.data.clamp_(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br> <br>            optimizer.zero_grad()<br>            model(input_img)<br>            style_score = <span class="hljs-number">0</span><br>            content_score = <span class="hljs-number">0</span><br> <br>            <span class="hljs-keyword">for</span> sl <span class="hljs-keyword">in</span> style_losses:<br>                style_score += sl.loss<br>            <span class="hljs-keyword">for</span> cl <span class="hljs-keyword">in</span> content_losses:<br>                content_score += cl.loss<br> <br>            style_score *= style_weight<br>            content_score *= content_weight<br> <br>            loss = style_score + content_score<br>            <span class="hljs-comment">#定义好损失函数，并计算完成（前向传播+计算loss完成）</span><br>        loss.backward()<br>            <span class="hljs-comment">#反向传递(计算待修改参数的梯度)</span><br>            run[<span class="hljs-number">0</span>] += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> run[<span class="hljs-number">0</span>] % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;run &#123;&#125;:&quot;</span>.<span class="hljs-built_in">format</span>(run))<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Style Loss : &#123;:4f&#125; Content Loss: &#123;:4f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                    style_score.item(), content_score.item()))<br>                <span class="hljs-built_in">print</span>()<br> <br>            <span class="hljs-keyword">return</span> style_score + content_score<br> <span class="hljs-comment">#closure里每次前向传播+计算loss+计算梯度 之后 -&gt; 优化器利用梯度更新参数即input_img</span><br>        optimizer.step(closure)<br> <br>    <span class="hljs-comment"># 最后的修正......</span><br>    input_img.data.clamp_(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br> <br>    <span class="hljs-keyword">return</span> input_img <br><span class="hljs-comment">#调用训练函数 并展示结果</span><br>output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,<br>                            content_img, style_img, input_img)<br> <br>plt.figure()<br>imshow(output, title=<span class="hljs-string">&#x27;Output Image&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>论文核心句子：</strong></p><p>the output of a given layer consists of so-called feature maps: differently filtered versions of the input image .</p><p>给定一个网络层，通过不同的卷积核过滤出不同的特征，多个卷积核过滤出的特征组合起来就是feature maps.</p>]]></content>
    
    
    <categories>
      
      <category>github</category>
      
    </categories>
    
    
    <tags>
      
      <tag>github</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git的简易使用手册</title>
    <link href="/2021/08/02/git%E7%9A%84%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    <url>/2021/08/02/git%E7%9A%84%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C/</url>
    
    <content type="html"><![CDATA[<h1 id="git的使用手册"><a class="markdownIt-Anchor" href="#git的使用手册"></a> git的使用手册</h1><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>背景知识介绍</font></center></td></tr></table><p>Git 是一个<strong>开源的分布式版本控制系统</strong>，用于敏捷高效地处理任何或小或大的项目（仓库）。有两种类型的仓库，即本地仓库和远程仓库。</p><p><strong>仓库类型</strong>：</p><ul><li>本地仓库：是在开发人员自己电脑尚的Git仓库</li><li>远程仓库：是在远程服务器上的Git仓库</li></ul><p><strong>主要操作类型</strong>：</p><ul><li>Clone：克隆，将远程仓库复制到本地（本地没有版本库，克隆整个版本库）</li><li>Push：推送，将本地仓库代码上传到远程仓库</li><li>Pull：拉取，将远程仓库代码下载到本地仓库（本地有版本库，从远程库获取最新commit数据（<u>如果有的话</u>），并merge到本地）</li></ul><p><strong>工作流程</strong>：</p><p>​        从远程仓库中克隆代码到本地仓库→从本地仓库中checkout代码然后进行代码修改→在提交前先将代码提交到暂存区→提交到本地仓库，本地仓库中保存修改的各个历史版本→修改完成后，需要和团队成员共享代码时，将代码push到远程仓库</p><pre><code class=" mermaid">graph LRA[remote]--&gt;|Clone|B[Repository]B[Repository]--&gt;|Push|A[remote]B[Repository]--&gt;|Checkout|C[Workspace]A[远程仓库]--&gt;|&quot;Pull(Fetch+Merge)&quot;|C[Workspace]D[暂存区]--&gt;|Commit|B[本地仓库]C[工作区]--&gt;|Add|D[暂缓区]style A fill:#f9f,stroke:#333,stroke-width:4px;style B fill:#f9f,stroke:#333,stroke-width:4px;style D fill:#f90,stroke:#555,stroke-width:4px;</code></pre><p><strong>工作区与暂缓区的区别？</strong></p><p><strong>工作区</strong>：仓库在电脑上的目录，比如目录下newrepo里的文件(.git隐藏目录版本库除外)。或者以后需要再新建的目录文件等等都属于工作区范畴。</p><p><strong>版本库(Repository)</strong>：工作区有一个隐藏目录.git,这个不属于工作区，这是版本库。其中版本库里面存了很多东西，其中最重要的就是stage(暂存区)，还有Git为我们自动创建了第一个分支master,以及指向master的一个指针HEAD。</p><p>我们前面说过使用Git提交文件到版本库有两步：<br /><strong>第一步</strong>：使用git add 把文件添加进去，实际上就是把文件添加到暂存区。<br /><strong>第二步</strong>：使用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支上。</p><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>使用说明</font></center></td></tr></table><p><strong>环境配置</strong>：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">设置用户信息<br><br>git config --global user.name “username”<br><br>git config --global user.email “emailname@163.com”<br><br>最终的信息会保存在~/.gitconfig文件中<br></code></pre></td></tr></table></figure><p><strong>获取Git仓库</strong>：</p><p>要使用Git对我们的代码进行版本控制，首先需要获取Git仓库，获取Git仓库的方式通常有两种：在本地初始化一个仓库以及从远程仓库克隆</p><p><font color= blue>在本地初始化一个Git仓库</font></p><ol><li>在电脑的任意位置创建一个空目录（例如newrepo）作为我们的本地Git仓库</li><li>进入这个目录中，点击右键打开Git bash窗口</li><li>执行命令git init</li></ol><p>如果在当前目录中看到.git文件夹（隐藏文件夹），则说明Git仓库<em>创建成功</em>。</p><p><img src="/img/git_1.png" alt="手动创建本地仓库" /></p><p><font color= blue>从远程仓库克隆</font></p><p>​通过Git提供的命令从远程仓库进行克隆，将远程仓库克隆到本地，命令形式为：<u>git clone 远程Git仓库地址</u>（从本地克隆 git clone /path/to/repository），远程Git地址一般用https://github.com/，如果报错可以换git@github.com: 试试。</p><p><strong>更新Git仓库：</strong></p><p>在版本库的目录（工作区）下新建一个记事本文件readme.txt</p><p><strong>第一步</strong>：使用命令 git add readme.txt添加到暂存区里面去。如下：</p><p><img src="/img/git_2.png" alt="第一步：工作区-&gt;暂存区" /></p><p>如果没有任何提示，说明已经添加成功了。</p><p><strong>第二步</strong>：用命令 git commit告诉Git，把文件提交到仓库</p><p><img src="/img/git_3.png" alt="第二步：暂存区-&gt;本地仓库" /></p><p>现在已经成功提交一个readme.txt文件了。</p><p><strong>第三步</strong>：检查是否还有文件未提交（比如修改了未提交等等），如下（手动修改readme.txt然后执行命令）：</p><p><img src="/img/git_4.png" alt="查看当前本地仓库的状态" /></p><p>上图表示readme.txt已经被修改，这时候需要检查修改的是什么内容，使用如下命令：</p><p><img src="/img/git_5.png" alt="查看工作区的文件修改内容" /></p><p>可以看到，readme文件从空字符变成两行数字，知道修改了什么内容就可以重复一二步进行commit，修改便成功提交。</p><pre><code class=" mermaid">graph LRA[工作区]--&gt;|git add &lt;filename&gt;|B[缓存区]B[缓存区]--&gt;|git commit -m &#x27;代码提交信息&#x27;|C[本地仓库]</code></pre><p><strong>撤销操作和删除文件</strong></p><p>当在工作区对文件进行操作修改后且在未提交（commit）之前，可以通过命令git checkout --readme.txt 把readme文件在工作区做的修改全部撤销，这时候分为<strong>两种情况</strong>：</p><ol><li>readme.txt修改后，还没有放到暂存区，使用撤销修改就回到和版本库一模一样的状态。</li><li>readme.txt已经放入暂存区了，接着又作了修改，撤销修改就回到添加暂存区后的状态。</li></ol><p>删除文件可直接rm b.txt，然后commit命令 提交掉，则彻底删除，提交之前可以通过checkout恢复该文件。</p><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>与远程仓库进行交互</font></center></td></tr></table><p>首先连接远程仓库，这个比较简单，windows和linux在网上都有简易教程，在此略过。大概过程为<font color=blue size=3>客户端输入ssh-keygen -t rsa -C “youremail@example.com” 一路回车→打开~/.ssh/id_rsa.pub并复制key→打开GitHub-Settings-SSH and GPG keys-New SSH key并粘贴保存即可.</font></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">邮箱和用户名已经在上面环境配置中设置好了<br>ssh -T git@github.com<br>输入该命令，紧接着输入密码，提醒successfully authenticated则认证成功<br></code></pre></td></tr></table></figure><p>保证可以连接远程仓库后，打开GitHub主页新建一个空的远程仓库比如newrepo，这时候可以在本地仓库的工作区输入以下命令进行提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">git remote -v 查看当前远程仓库<br>git remote rm origin 有远程仓库先删掉<br>git remote add origin git@github.com:hnurxn(username)/newrepo(projectname).git 重新设置为你的用户名/仓库名<br>git branch -M main(可有可无默认为master，push的分支与下面统一)<br>git push -u origin master 完成push操作<br></code></pre></td></tr></table></figure><p><img src="/img/git_6.png" alt="本地向远程传送" /></p><p>以上是最基本的从本地到远程服务器push文件的方法，细节操作有以下内容：</p><p>查看当前的远程库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git remote<br>origin<br>$ git remote -v<br>origin    git@github.com:tianqixin/runoob-git-test.git (fetch)<br>origin    git@github.com:tianqixin/runoob-git-test.git (push)<br>执行时加上-v参数，可以看到每个别名的实际链接地址<br></code></pre></td></tr></table></figure><p>提取远程仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">1.从远程仓库下载新分支与数据<br>$ git fetch<br>2.然后执行git merge远程分支到你所在的分支<br>$ git merge<br>该命令就是在执行 git fetch 之后紧接着执行 git pull 远程分支到你所在的任意分支。<br>假设你配置好了一个远程仓库，并且你想要提取更新的数据，你可以首先执行 git fetch [<span class="hljs-built_in">alias</span>] 告诉 Git 去获取它有你没有的数据，然后你可以执行 git merge [<span class="hljs-built_in">alias</span>]/[branch] 以将服务器上的任何更新（假设有人这时候推送到服务器了）合并到你的当前分支。<br></code></pre></td></tr></table></figure><p>推送到远程仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">推送你的新分支与数据到某个远端仓库<br>$ git push [<span class="hljs-built_in">alias</span>] [branch]<br>以上命令将你的 [branch] 分支推送成为 [<span class="hljs-built_in">alias</span>] 远程仓库上的 [branch] 分支，实例如下:<br>$ git push origin master <br>将本地的master分支推送到origin远程仓库上<br></code></pre></td></tr></table></figure><p>删除远程仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git remote -v<br>origin    git@github.com:tianqixin/runoob-git-test.git (fetch)<br>origin    git@github.com:tianqixin/runoob-git-test.git (push)<br><br><span class="hljs-comment"># 添加仓库 origin2</span><br>$ git remote add origin2 git@github.com:tianqixin/runoob-git-test.git<br><br>$ git remote -v<br>origin    git@github.com:tianqixin/runoob-git-test.git (fetch)<br>origin    git@github.com:tianqixin/runoob-git-test.git (push)<br>origin2    git@github.com:tianqixin/runoob-git-test.git (fetch)<br>origin2    git@github.com:tianqixin/runoob-git-test.git (push)<br><br><span class="hljs-comment"># 删除仓库 origin2</span><br>$ git remote rm origin2<br>$ git remote -v<br>origin    git@github.com:tianqixin/runoob-git-test.git (fetch)<br>origin    git@github.com:tianqixin/runoob-git-test.git (push)<br></code></pre></td></tr></table></figure><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>分支的使用</font></center></td></tr></table><p>分支即branch，每个分支可以理解为独立的一个项目思路。每个branch有自己独立的工作区，即有独立的文件区，在本地仓库下输入以下命令进行操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ git branch 列举分支<br>$ git branch testing 新建分支<br>$ git checkout testing 切换当前分支<br></code></pre></td></tr></table></figure><p>以从远程仓库clone到本地仓库为例：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash">1.先将远程仓库克隆到本地<br>$ git <span class="hljs-built_in">clone</span> 仓库地址<br>2.在本地仓库打开Git bash 默认的分支为master<br>3.利用checkout切换分支（如果远程仓库有多个分支的话）<br>$ git checkout branchname<br>4.切换到哪个分支，文件夹则显示哪个分支的文件区，即当前工作区（每个分支有不同的提交记录commit），在当前工作区可以修改文件，然后add+commit,commit记录保存到该分支的总记录中。<br>5.在该分支下，输入 git push -u origin branchname 则将该分支push到远程仓库的对应分支<br></code></pre></td></tr></table></figure><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>容易混淆的概念</font></center></td></tr></table><p><font color = blue size = 4>工作区 本地仓库  远程仓库  项目  版本库 仓库</font></p><p><strong>工作区</strong>是指<strong>本地仓库</strong>除去.git文件夹的全部内容，即具体的代码工程，每一次修改与提交都会记录在.git文件夹里，每一次修改也代表着一个新的版本，因此仓库整体又称为<strong>版本库</strong></p><p><strong>项目+git版本控制=仓库=版本库（本地仓库+远程仓库）=工作区+.git文件夹</strong></p><table><tr><td bgcolor=yellow><center><font face="黑体" size=5>下载GitHub的单个文件</font></center></td></tr></table><p><strong>方法一</strong>：</p><p>假设 GitHub 文件的原 URL 是：</p><p><a href="https://github.com/keras-team/keras/blob/master/keras/layers/preprocessing/image_preprocessing.py">https://github.com/keras-team/keras/blob/master/keras/layers/preprocessing/image_preprocessing.py</a></p><p>将其更改为：</p><p><a href="https://raw.githubusercontent.com/keras-team/keras/master/keras/layers/preprocessing/image_preprocessing.py">https://raw.githubusercontent.com/keras-team/keras/master/keras/layers/preprocessing/image_preprocessing.py</a></p><p>具体操作步骤为：</p><ul><li>【<a href="http://github.com">github.com</a>】→ 【<a href="http://raw.githubusercontent.com">raw.githubusercontent.com</a>】</li><li>【blob】→【去掉】</li><li>wget  [更改后的url]</li></ul><p><strong>方法二：</strong></p><p>首先安装SVN，Linux用如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">sudo apt install subversion<br></code></pre></td></tr></table></figure><p>执行如下命令，用来列举url下面的所有目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">svn ls https://github.com/MiCode/Xiaomi_Kernel_OpenSource.git<br></code></pre></td></tr></table></figure><p>根据下面的目录 可以继续做深度搜索 ，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">svn ls https://github.com/MiCode/Xiaomi_Kernel_OpenSource.git/branches<br></code></pre></td></tr></table></figure><p>一直到找到想要下载的文件路径，用export操作如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">svn <span class="hljs-built_in">export</span> https://github.com/MiCode/Xiaomi_Kernel_OpenSource.git/branches/laurus-p-oss/drivers/input/fingerprint/gf_spi.c<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Tutorials</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Typora的安装</title>
    <link href="/2021/07/13/Typora%E5%AE%89%E8%A3%85/"/>
    <url>/2021/07/13/Typora%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h1 id="typora安装"><a class="markdownIt-Anchor" href="#typora安装"></a> Typora安装</h1><p>Typora可以用markdown语法编写post，可以导出多种格式的文件，图片可以直接拖拽，非常方便。也可以使用latex的数学公式语法。</p><p>下载地址：</p><p><a href="https://www.typora.io/#windows">https://www.typora.io/#windows</a></p><p>选择要安装的路径，直接安装即可。😄</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi mathvariant="normal">/</mi><mi>p</mi><mi>i</mi></mrow></msup><mo>=</mo><mi>s</mi><mi>i</mi><mi>n</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">e^{i/pi} = sin x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.938em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mtight">/</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">i</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">x</span></span></span></span></span></p><pre><code class=" mermaid">graph TD;    A--&gt;B;    A--&gt;C;    B--&gt;D</code></pre><pre><code class=" mermaid">sequenceDiagram    Alice-&gt;&gt;John: Hello John, how are you?    John--&gt;&gt;Alice: Great!</code></pre><h2 id="git和nodejs的下载"><a class="markdownIt-Anchor" href="#git和nodejs的下载"></a> git和node.js的下载</h2><p>均去官网下载即可。</p><ul><li>Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。</li><li>Node.js 就是运行在服务端的 JavaScript。Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。</li><li><strong>检验安装成功的标准：在cmd窗口输入git，以及输入node-v和npm -v结果显示相应内容则安装成功。</strong></li></ul><p>下载好git和nodejs就可以使用Hexo，Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。</p><h2 id="hexo的下载安装"><a class="markdownIt-Anchor" href="#hexo的下载安装"></a> Hexo的下载安装</h2><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">npm</span> install -g hexo-cli <br></code></pre></td></tr></table></figure><p>同上，输入hexo -v 验证是否安装成功。然后新建一个文件夹blogs（存hexo代码），进入这个文件夹，hexo init初始化，然后再输入npm install。文件建立完成后，指定文件夹目录关键的有themes存放主题，source存放文章等。最后再hexo g 就完成了.hexo s是开启服务器的命令，输入后便可以在localhost:4000访问了，可以加-p 4000修改端口。</p><p>总之，nodejs环境下载好，就可以<u>在本机上运行hexo服务</u>，也就是可以写博客，在本机浏览器上访问，<u>git的作用就是把hexo部署到github上，部署成功好就可以用name.github.io来访问</u>。</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;note note-primary&quot;</span>&gt;</span>标签<span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="git配置过程"><a class="markdownIt-Anchor" href="#git配置过程"></a> git配置过程</h4><ol><li><p>首先进入github官网注册一个账号，账号的两个标志就是用户名和邮箱</p></li><li><p>新建一个仓库，<a href="http://xn--siqu5lba675eq1iit3a.github.io">名字为用户名.github.io</a>，<a href="http://xn--rxn-eo8er22f.github.io">比如rxn.github.io</a></p></li><li><p>在根目录git bash窗口输入以下代码，让本机记住你的GitHub账号</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">git config --global user.name <span class="hljs-string">&quot;yourname&quot;</span>//yourname填写你的github用户名<br>git config --global user.email <span class="hljs-string">&quot;youremail&quot;</span>//youremail填写你的github的邮箱<br><br></code></pre></td></tr></table></figure></li><li><p>创造本机和github服务器的连接，通过ssh-keygen -t rsa -C “youremail” 存储路径和密码以及确认密码（159352），打开本机用户下.ssh的id_rsa.pub，将里面的内容粘贴到（GitHub主页右上角头像-&gt;SSH and GPG keys-&gt; new SSH key -&gt; key ），title内容随便填。</p></li><li><p>这时你的GitHub账号和本机git互相识别，输入 ssh -T <a href="mailto:git@github.com">git@github.com</a> 就会连接成功，git可以正常给GitHub服务器上传以及下载内容。</p></li><li><p>在name.github.io项目中，可以通过settings来选择以哪个分支为主目录，从而作为自己的主页</p><h4 id="hexo配置过程"><a class="markdownIt-Anchor" href="#hexo配置过程"></a> hexo配置过程</h4><p>将hexo和GitHub关联起来，在博客根目录下找到_config.yml，在最下面找到</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">deploy:<br>  <span class="hljs-built_in">type</span>: git<br>  repo:git@github.com:yourname/yourname.github.io.git <br>  branch: master<br></code></pre></td></tr></table></figure><p>repo的yourname是github用户名，然后在根目录下的git bash中输入：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">npm install hexo-deployer-git --same <span class="hljs-comment">#安装deploy-git</span><br>hexo clean<br>hexo g  <span class="hljs-comment">#生成相关html文件</span><br>hexo d  <span class="hljs-comment">#部署到GitHub上</span><br></code></pre></td></tr></table></figure><p>这个时候在GitHub仓库中可以发现和本地博客目录一样的文件，打开浏览器，输入xxxx（用户名）.github.io，就可以访问博客了。</p></li></ol><h3 id="hexo的使用"><a class="markdownIt-Anchor" href="#hexo的使用"></a> hexo的使用</h3><p>​新建博客，可以手动创建md文件，也可以通过以下命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&#x27;博客名字&#x27;</span> <span class="hljs-comment">#会添加到source/_post里</span><br>hexo d -g <span class="hljs-comment">#提交到GitHub上</span><br><br>hexo generate <span class="hljs-comment">#生成静态页面至public目录</span><br>hexo server <span class="hljs-comment">#开启预览访问端口（默认端口4000，&#x27;ctrl + c&#x27;关闭server）</span><br>hexo deploy <span class="hljs-comment">#部署到GitHub</span><br>hexo <span class="hljs-built_in">help</span>  <span class="hljs-comment"># 查看帮助</span><br>hexo version  <span class="hljs-comment">#查看Hexo的版本</span><br>hexo s -g <span class="hljs-comment">#生成并本地预览</span><br>hexo d -g <span class="hljs-comment">#生成并上传</span><br></code></pre></td></tr></table></figure><p>更换主题，以yilia主题为例子（<u>在hexo根目录执行git bash命令，下载的所有主题都在/themes目录中</u>）：</p><p>​<code>$ git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia</code> 下载主题到themes文件夹，然后修改根目录的_confg.yml，修改为theme：yilia.</p><p>hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流</p><p><strong>1.hexo的基本配置</strong></p><p><a href="https://blog.csdn.net/sinat_37781304/article/details/82729029">参考连接</a></p><p>安装git（官网下载）-&gt;node（官网下载）-&gt;hexo（npm install hexo-cli -g）-&gt;hexo init <folder>-&gt;cd <floder>（新建一个存放博客的文件夹folder，git bash环境中操作）-&gt; git clone sth(下载一些主题放在theme里) and npm install  -&gt;hexo s（本地测试）/配置里修改deploy属性-&gt;hexo clean -&gt; hexo g -&gt; hexo d 传到GitHub上</p><p><a href="https://hexo.fluid-dev.com/docs/guide/"> fluid官方配置教程</a>(tag和数学公式有点小问题-在后面百分之80的位置)</p><p>fluid具体内容配置要点</p>]]></content>
    
    
    <categories>
      
      <category>Tutorials</category>
      
      <category>install</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>nodejs</tag>
      
      <tag>git</tag>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2021/07/12/hello-world/"/>
    <url>/2021/07/12/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start"><a class="markdownIt-Anchor" href="#quick-start"></a> Quick Start</h2><h3 id="create-a-new-post"><a class="markdownIt-Anchor" href="#create-a-new-post"></a> Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="参考资料《hexo博客的基础搭建》">[1]</span></a></sup> info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server"><a class="markdownIt-Anchor" href="#run-server"></a> Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files"><a class="markdownIt-Anchor" href="#generate-static-files"></a> Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites"><a class="markdownIt-Anchor" href="#deploy-to-remote-sites"></a> Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><h3 id="how-to-use-image"><a class="markdownIt-Anchor" href="#how-to-use-image"></a> how to use Image</h3><h3 id=""><a class="markdownIt-Anchor" href="#"></a> </h3><p><img src="/img/example.jpg" alt="王者荣耀女武神" /></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs"><br></code></pre></td></tr></table></figure><h2 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h2><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>参考资料《hexo博客的基础搭建》<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>参考文章《深度学习的基本使用》<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>Tutorials</category>
      
      <category>start</category>
      
    </categories>
    
    
    <tags>
      
      <tag>tutorial</tag>
      
      <tag>quick start</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
