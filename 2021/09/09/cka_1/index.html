

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.png">
  <link rel="icon" href="/img/icon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="cka是一个可以度量单独网络层之间的相似性的指标，适用于各种网络，可以帮助更好地理解神经网络以及改进网络">
  <meta name="author" content="Mr Free">
  <meta name="keywords" content="">
  
  <title>cka指标如何度量神经网络的相似性 - hnurxn&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.7.2/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":60,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"e6b05da17eecff6523584dd51e7da014","google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"MJAd57UreVBPxte7PDoEMye0-gzGzoHsz","app_key":"388yI2KkDdst7lb1E99J416E","server_url":"https://mjad57ur.lc-cn-n1-shared.com"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>一只咸鱼的日常</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/background.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="cka指标如何度量神经网络的相似性">
              
            </span>

            
              <div class="mt-3">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-author" aria-hidden="true"></i>
      Mr Free
    </span>
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-09-09 20:39" pubdate>
        2021年9月9日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4.1k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      63
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
    <a target="_blank" rel="noopener" href="https://your-url" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">cka指标如何度量神经网络的相似性</h1>
            
            <div class="markdown-body">
              <p>[TOC]</p>
<h1 id="overview"><a class="markdownIt-Anchor" href="#overview"></a> Overview</h1>
<p><font color='red' size =4>首先用论文里的一句话引出主题: </font></p>
<p>如何确定两个神经网络学习到的内容是否相同？不同深度以及宽度的神经网络学到的内容有何区别？</p>
<blockquote>
<p><em>Deep neural network architectures are typically tailored to available computational resources by scaling their width and/or depth. Remarkably, this simple approach to model scaling can result in state-of-the-art networks for both high- and low-resource regimes (Tan &amp; Le, 2019)</em></p>
</blockquote>
<p>本篇文章主要分为以下几部分：</p>
<ul>
<li>背景介绍</li>
<li>构造网络</li>
<li>解释cka公式</li>
<li>编写代码且可视化</li>
</ul>
<h1 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h1>
<blockquote>
<p><em>We apply CKA (centered kernel alignment) to measure the similarity of the hidden representations of different neural network architectures, finding that representations in wide or deep models exhibit a characteristic structure, which we term the</em> <em>block structure. We study how the block structure varies across different training runs, and uncover a connection between block structure and model overparametrization — block structure primarily appears in overparameterized model</em></p>
</blockquote>
<p><img src="D:%5Cstudy%5Cgit%5Cmyblog%5Csource%5Cimg%5Clab%5Ccka%E5%8A%A8%E5%9B%BE.gif" srcset="/img/loading.gif" lazyload alt="cka动图" /></p>
<p>CKA用来测量不同神经网络的隐藏层之间的相似度，即比较两个网络的activations。</p>
<h1 id="experimental-setup"><a class="markdownIt-Anchor" href="#experimental-setup"></a> Experimental setup</h1>
<blockquote>
<p><em>our experimental setup consists of a family of ResNets (He et al., 2016; Zagoruyko &amp; Komodakis, 2016) trained on standard image classification datasets CIFAR-10, CIFAR-100 and ImageNe</em>t.</p>
</blockquote>
<p>为了简单起见，而且要有代表性，本次实验专注于ResNets:50,101,152 网络和CIFAR-10 数据集。</p>
<p>最小的Resnet网络可以很容易地实例化，通过从keras下载模型，并加载在imageNet上预训练的权重。<strong>我们将输入的shape设置为 (32,32,3)</strong>，即 <strong>CIFAR-10</strong> 数据集对应的尺寸。 我们不需要下载全连接的输出层，因此我们将 include_top 设置为 False。 最后，我们想要池化输出层，它是一个 4D 张量，池化后便可从基础模型中获得 2D 输出。 由于我们的数据集中有 10 个类，因此我们将自己的 <strong>Dense 输出层包含在 10 个神经元和 softmax 激活</strong>中。<br />
最后，我们使用**<font color = blue>基础模型的输入作为输入</font><strong>，</strong><font color=blue>Dense 层作为输出</font>**来创建一个模型实例。</p>
<p>以下是ResNet[50,101,152]的构建过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_resnet50</span>():</span><br>    resnet_base = tf.keras.applications.ResNet50(input_shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>),<br>                                                 weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>,<br>                                                 pooling=<span class="hljs-string">&#x27;avg&#x27;</span>,<br>                                                 include_top=<span class="hljs-literal">False</span>)<br>    output = tf.keras.layers.Dense(<span class="hljs-number">10</span>,<br>                                   activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(resnet_base.output)<br>    model = tf.keras.Model(inputs=[resnet_base.<span class="hljs-built_in">input</span>], outputs=[output])<br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_resnet101</span>():</span><br><br>    resnet_base = tf.keras.applications.ResNet101(input_shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>),<br>                                                  weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>,<br>                                                  pooling=<span class="hljs-string">&#x27;avg&#x27;</span>,<br>                                                  include_top=<span class="hljs-literal">False</span>)<br><br>    output = tf.keras.layers.Dense(<span class="hljs-number">10</span>,<br>                                   activation=<span class="hljs-string">&quot;softmax&quot;</span>)(resnet_base.output)<br><br>    model = tf.keras.Model(inputs=[resnet_base.<span class="hljs-built_in">input</span>], outputs=[output])<br><br>    <span class="hljs-keyword">return</span> model<br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_resnet152</span>():</span><br><br>    resnet_base = tf.keras.applications.ResNet152(input_shape=(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>),<br>                                                  weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>,<br>                                                  pooling=<span class="hljs-string">&#x27;avg&#x27;</span>,<br>                                                  include_top=<span class="hljs-literal">False</span>)<br><br>    output = tf.keras.layers.Dense(<span class="hljs-number">10</span>,<br>                                   activation=<span class="hljs-string">&quot;softmax&quot;</span>)(resnet_base.output)<br><br>    model = tf.keras.Model(inputs=[resnet_base.<span class="hljs-built_in">input</span>], outputs=[output])<br><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure>
<p>获取CIFAR-10 数据集比较简单，直接通过tensorflow下载即可，并简单做归一化预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">cifar10 = tf.keras.datasets.cifar10<br><br>(x_train, y_train), (x_test, y_test) = cifar10.load_data()<br>x_train, x_test = x_train / <span class="hljs-number">255.0</span>, x_test / <span class="hljs-number">255.0</span> <span class="hljs-comment">#rescale the data</span><br></code></pre></td></tr></table></figure>
<h1 id="measuring-similarity"><a class="markdownIt-Anchor" href="#measuring-similarity"></a> Measuring similarity</h1>
<blockquote>
<p><em>We use linear centered kernel alignment (Kornblith et al., 2019; Cortes et al., 2012) to measure similarity between neural network hidden representations</em></p>
</blockquote>
<p>如何衡量两个神经网络之间的相似性？一个直观的方法就是计算隐藏层的输出之间的相似度。具体执行也比较简单，给定一个小批次数据，不仅捕获最终输出也要捕获所有隐藏层的输出。对给定两个网络A和B做同样的操作就可以得到两个activations集合 <font color = red size =4>A[hidden_layer1_out,hidden_layer2_out,…,final_out] &amp; B[hidden_layer1_out,hidden_layer2_out,…,final_out]</font></p>
<img src="D:\study\git\myblog\source\img\lab\representation.png" srcset="/img/loading.gif" lazyload alt="representation" style="zoom: 33%;" />
<p>对两个集合的元素成对比较，得到每个层对（pairs）的相似度，这便完成了整个网络的相似度计算。但是，对相似度指标有一些基本要求，<strong>范围是[0,1]</strong>,（0表示两个完全不同的激活矩阵，1表示完全相同），除此之外，也要<strong>能处理shape不一致的两个网络</strong>。而满足这个要求的指标则是<strong>CKA（ centered kernel alignment ）</strong></p>
<blockquote>
<p><em>To reduce memory consumption, we compute CKA as a function of average HSIC scores computed over k minibatches:</em></p>
</blockquote>
<img src="D:\study\git\myblog\source\img\lab\cka公式.png" srcset="/img/loading.gif" lazyload alt="cka公式" style="zoom: 50%;" />
<blockquote>
<p><em>where Xᵢ ∈ R^(n x p₁) and Yᵢ ∈ R^(n x p₂) are matrices containing the activations of two layers, one with p₁ neurons and another p₂ neurons, to the same minibatch of n examples sampled without replacement</em></p>
</blockquote>
<p>当对足够多的epoch进行平均时，该minibatch计算法得到的结果与整个数据集计算HSIC1得到的结果相同。<u>论文将batch设为256，并迭代10次，计算结果</u>。</p>
<p><strong><font color = #00FF00>注：Sampling Without Replacement：无放回抽样  U-statistic U-统计量</font></strong></p>
<p>首先得介绍一下HSIC，在这里用无偏的计算方法，独立于batch size</p>
<img src="D:\study\git\myblog\source\img\lab\HSIC有偏公式.png" srcset="/img/loading.gif" lazyload alt="image-20210811221701108" style="zoom: 50%;" />
<p>该公式时有偏的HSIC计算过程（列举出来便于对比学习），下面时HSIC的无偏计算过程</p>
<blockquote>
<p><em>We use an unbiased estimator of HSIC (Song et al., 2012) so that the value of CKA is independent of the batch size</em></p>
</blockquote>
<img src="D:\study\git\myblog\source\img\lab\HSIC公式.png" srcset="/img/loading.gif" lazyload alt="image-20210811221441860" style="zoom:50%;" />
<blockquote>
<p><em>where ~K and ~L are obtained by setting the diagonal entries of K and L to zero</em></p>
</blockquote>
<p><strong>接下来详细讲解一下该公式：</strong></p>
<p>HSIC 是 Hilbert-Schmid-Independence Criterion 的缩写，它衡量两个分布之间的统计独立性。 在我们的例子中，网络层的激活值便是这里的分布。</p>
<p>让我们再次回顾HSIC1的公式，在这里补充几个数学知识，<strong>tr()操作</strong>是计算矩阵的迹(trace)，迹指的是矩阵对角线的元素之和。<strong>K和L都被加粗</strong>，意思是<strong>K</strong>和<strong>L</strong>都是矩阵，这也是约定俗成的东西。矩阵上面<strong>加波浪线</strong>一般指的增广矩阵，即与原矩阵不同的矩阵，加点标记以示不同。</p>
<blockquote>
<p><em>We use minibatches of size n = 256 obtained by iterating over the test dataset 10 times, sampling without replacement within each epoch</em></p>
</blockquote>
<p><font color = blue>这表示我们的矩阵将从<strong>shape (256,a*b*c…)</strong> 到 **shape (256,256)**的方阵(square matrix)。</font></p>
<hr />
<p>现在开始一步一步了解 | refer to 《<strong>Similarity of Neural Network Representations Revisited</strong>》</p>
<center><font color=red size = 6>CKA的进化史</font></center>
\langle\text{vec}(XX^\text{T}),\text{vec}(YY^\text{T})\rangle =\text{tr}(XX^\text{T}YY^\text{T}) = ||Y^\text{T}X||_\text{F}^2 \tag{1} \label{eq1}

<blockquote>
<p>The left-hand side of (1) thus measures the similarity between the <strong>inter-example similarity</strong> structures. The right-hand side yields the same result by measuring the <strong>similarity between features from X and Y</strong> , by summing the squared dot products between every pair.</p>
</blockquote>
<p>在公式\eqref{eq1}中可以看到神奇的事情，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>​分别与各自的转置相乘(得到inter-example的相似度)然后再作点积的结果和两者的协方差矩阵的F范数（得到两者各特征对的相似度）<strong>结果出奇的一致</strong>！于是激发起作者要使用前者来计算相似度的idea.</p>
<table><tr><td bgcolor=yellow><center><font face="黑体" size=5>预处理-centered matrix中心化矩阵【1】</font></center></td></tr></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#中心化矩阵  比如X是n*p的矩阵  中心化则 p列 每列都减去该列的平均值</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">centering_c</span>(<span class="hljs-params">K</span>):</span><br>    n = K.shape[<span class="hljs-number">0</span>]<br>    unit = np.ones([n, n])<br>    I = np.eye(n)<br>    H = I - unit / n<br><br>    <span class="hljs-keyword">return</span> np.dot(H,K)<br></code></pre></td></tr></table></figure>
<blockquote>
<p>We assume that these matrices have been preprocessed to center the columns.</p>
</blockquote>
<p>这一步是完成对数据的centered，即所有特征去均值中心化。</p>
<table><tr><td bgcolor=yellow><center><font face="黑体" size=5>计算核函数-gram matrix【2】</font></center></td></tr></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#线性核</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gram_linear</span>(<span class="hljs-params">x</span>):</span><br>  <span class="hljs-string">&quot;&quot;&quot;Compute Gram (kernel) matrix for a linear kernel.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Args:</span><br><span class="hljs-string">    x: A num_examples x num_features matrix of features.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Returns:</span><br><span class="hljs-string">    A num_examples x num_examples Gram matrix of examples.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-keyword">return</span> x.dot(x.T)<br></code></pre></td></tr></table></figure>
<table><tr><td bgcolor=yellow><center><font face="黑体" size=5>中心化gram matrix【3】</font></center></td></tr></table>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">center_gram</span>(<span class="hljs-params">gram, unbiased=<span class="hljs-literal">False</span></span>):</span><br>  <span class="hljs-string">&quot;&quot;&quot;Center a symmetric Gram matrix.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  This is equvialent to centering the (possibly infinite-dimensional) features</span><br><span class="hljs-string">  induced by the kernel before computing the Gram matrix.</span><br><span class="hljs-string">  先将X和Y矩阵特征中心化后求协方差矩阵</span><br><span class="hljs-string">  对于核计算，可以先特征中心化，也可以核计算后再进行该函数操作，等价于核计算之前先将X和Y的特征center （对于线性核测试通过，RBF核测试不通过） 因为协方差可以与线性核等价</span><br><span class="hljs-string">  Args:</span><br><span class="hljs-string">    gram: A num_examples x num_examples symmetric matrix.</span><br><span class="hljs-string">    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased</span><br><span class="hljs-string">      estimate of HSIC. Note that this estimator may be negative.可能为负</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Returns:</span><br><span class="hljs-string">    A symmetric matrix with centered columns and rows.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> np.allclose(gram, gram.T):<br>    <br>    <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Input must be a symmetric matrix.&#x27;</span>)<br>  gram = gram.copy()<br><br>  <span class="hljs-keyword">if</span> unbiased:<br>    <span class="hljs-comment"># This formulation of the U-statistic, from Szekely, G. J., &amp; Rizzo, M.</span><br>    <span class="hljs-comment"># L. (2014). Partial distance correlation with methods for dissimilarities.</span><br>    <span class="hljs-comment"># The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically</span><br>    <span class="hljs-comment"># stable than the alternative from Song et al. (2007).</span><br>    n = gram.shape[<span class="hljs-number">0</span>]<br>    np.fill_diagonal(gram, <span class="hljs-number">0</span>)<br>    means = np.<span class="hljs-built_in">sum</span>(gram, <span class="hljs-number">0</span>, dtype=np.float64) / (n - <span class="hljs-number">2</span>)<br>    means -= np.<span class="hljs-built_in">sum</span>(means) / (<span class="hljs-number">2</span> * (n - <span class="hljs-number">1</span>))<br>    gram -= means[:, <span class="hljs-literal">None</span>]<br>    gram -= means[<span class="hljs-literal">None</span>, :]<br>    np.fill_diagonal(gram, <span class="hljs-number">0</span>)<br>  <span class="hljs-keyword">else</span>:<br>    means = np.mean(gram, <span class="hljs-number">0</span>, dtype=np.float64)<span class="hljs-comment"># column</span><br>    means -= np.mean(means) / <span class="hljs-number">2</span><br>    gram -= means[:, <span class="hljs-literal">None</span>]<br>    gram -= means[<span class="hljs-literal">None</span>, :]<br><br>  <span class="hljs-keyword">return</span> gram<br></code></pre></td></tr></table></figure>
<p>公式\eqref{eq1}左边的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi><msup><mi>X</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">XX^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><msup><mi>Y</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">YY^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>​​​​的计算过程，通过以上两个步骤的组合就可以计算两个activations的相似度了。<font color=red>【1】+【2】/【2】+【3】</font></p>
<p>可以先作中心化预处理再计算gram矩阵，也可以直接计算gram矩阵，再通过矩阵运算作中心化处理，本质上相同。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi mathvariant="normal">_</mi><mi>c</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mi>c</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi mathvariant="normal">_</mi><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo stretchy="false">(</mo><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi mathvariant="normal">_</mi><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">gram\_linear(centering\_c(X))=center\_gram(gram\_linear(X))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault">c</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">e</span><span class="mord mathdefault">n</span><span class="mord mathdefault">t</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord mathdefault">a</span><span class="mord mathdefault">m</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mord mathdefault">e</span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>目前可以通过两种方法得到待比较<strong>网络层 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span> 激活层中心化后的gram矩阵</strong>，即得到了公式\eqref{eq1}中的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>tr</mtext><mo stretchy="false">(</mo><mi>X</mi><msup><mi>X</mi><mtext>T</mtext></msup><mi>Y</mi><msup><mi>Y</mi><mtext>T</mtext></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{tr}(XX^\text{T}YY^\text{T})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">tr</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">T</span></span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord text mtight"><span class="mord mtight">T</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>​</p>
\frac{1}{(n-1)^2}\text{tr}(XX^\text{T}YY^\text{T})=||cov(X^\text{T}Y^\text{T})||^2_\text{F} \tag{2} \label{eq2}

<blockquote>
<p>The Hilbert-Schmidt Independence Criterion (Gretton et al., 2005) generalizes Equations 1 and 2 to inner products from reproducing kernel Hilbert spaces, where the squared Frobenius norm of the cross-covariance matrix becomes the squared Hilbert-Schmidt norm of the cross-covariance operator.</p>
</blockquote>
\text{HSIC}(K,L)=\frac{1}{(n-1)^2}\text{tr}(KHLH) \tag{3} \label{eq3}

<p>最终我们的表示形式如公式\eqref{eq3}​​所示，作为我们的相似度指标，而且上面代码已经将<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>tr</mtext><mo stretchy="false">(</mo><mi>K</mi><mi>H</mi><mi>L</mi><mi>H</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{tr}(KHLH)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">tr</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mord mathdefault">L</span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mclose">)</span></span></span></span>​​​算出来了，接下来问题又来了，这个指标不满足同性放缩不变性，可以<strong>用归一化方法让它满足同性放缩不变化</strong>。</p>
<blockquote>
<p>HSIC is not invariant to isotropic scaling, but it can be made invariant through normalization. This normalized index is known as centered kernel alignment (Cortes et al., 2012; Cristianini et al., 2002).</p>
</blockquote>
\text{CKA}(K,L)=\frac{\text{HSIC}(K,L)}{\sqrt{\text{HSIC}(K,K)\text{HSIC}(L,L)}} \tag{4} \label{eq4}

<p>以上则是《<strong>Similarity of Neural Network Representations Revisited</strong>》论文里的核心内容，最终的相似度指标。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cka</span>(<span class="hljs-params">gram_x, gram_y, debiased=<span class="hljs-literal">False</span></span>):</span><br>  <span class="hljs-string">&quot;&quot;&quot;Compute CKA.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Args:</span><br><span class="hljs-string">    gram_x: A num_examples x num_examples Gram matrix.</span><br><span class="hljs-string">    gram_y: A num_examples x num_examples Gram matrix.</span><br><span class="hljs-string">    debiased: Use unbiased estimator of HSIC. CKA may still be biased.</span><br><span class="hljs-string"></span><br><span class="hljs-string">  Returns:</span><br><span class="hljs-string">    The value of CKA between X and Y.</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br>  gram_x = center_gram(gram_x, unbiased=debiased)<br>  gram_y = center_gram(gram_y, unbiased=debiased)<br><br>  <span class="hljs-comment"># Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or</span><br>  <span class="hljs-comment"># n*(n-3) (unbiased variant), but this cancels抵消 for CKA.</span><br>  scaled_hsic = gram_x.ravel().dot(gram_y.ravel()) <br>  <span class="hljs-comment">#&lt;vec(X,X.T),vec(Y,Y.T)&gt; 等价于 Tr(X,X.T,Y,Y.T)  等价于||Y.T,X||2/F</span><br>  normalization_x = np.linalg.norm(gram_x)  <span class="hljs-comment">#||X.T,X||2/F = HSIC(K,K)</span><br>  normalization_y = np.linalg.norm(gram_y)  <span class="hljs-comment">#||Y.T,Y||2/F = HSIC(L,L)</span><br></code></pre></td></tr></table></figure>
<p>此代码则是公式\eqref{eq4}​​的实现，计算gram矩阵使用的是【2】+【3】即<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mtext>cka</mtext><mo stretchy="false">(</mo><mtext>gram_linear</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext>gram_linear</mtext><mo stretchy="false">(</mo><mi>Y</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{cka}(\text{gram\_linear}(X),\text{gram\_linear}(Y))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord text"><span class="mord">cka</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">gram_linear</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">gram_linear</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>​​​,<strong>默认参数使用的是有偏估计，如果需要有偏估计则将参数debiased设为True.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cka_linear_single</span>(<span class="hljs-params">net1,net2,debiased = <span class="hljs-literal">False</span></span>):</span><br>    l1 = <span class="hljs-built_in">len</span>(net1)<br>    l2 = <span class="hljs-built_in">len</span>(net2)<br>    ans = np.zeros((l1,l2),dtype=<span class="hljs-built_in">float</span>)<br>    batch_size = net1[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]<br>    g1 = []<br>    g2 = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,l1):<br>        g1.append(gram_linear(net1[i].reshape(batch_size,-<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,l2):<br>        g2.append(gram_linear(net2[i].reshape(batch_size,-<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,l1):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,l2):<br>            ans[l1-i-<span class="hljs-number">1</span>][j] = cka(g1[i],g2[j],debiased)<br>    <span class="hljs-keyword">return</span> ans<br></code></pre></td></tr></table></figure>
<p>以上代码则是对cka的包装，输入为两个网络Net1和Net2，每个网络是一个装有有限个网络层的列表[layer_1,…,layer_n]，返回值是矩阵</p>
<p>现在开始简单介绍节省内存的计算方法 | refer to 《<strong>Similarity of Neural Network Representations Revisited</strong>》</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">unbiased_HSIC</span>(<span class="hljs-params">K, L</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;计算HISC的无偏估计unbaised estimator of HISC &#x27;&#x27;&#x27;</span><br><br>    <span class="hljs-comment"># create the unit vector filled with ones</span><br>    n = K.shape[<span class="hljs-number">0</span>]<br>    ones = np.ones(shape=(n))<br><br>    <span class="hljs-comment">#fill the diagonal entries with zeros</span><br>    np.fill_diagonal(K, val=<span class="hljs-number">0</span>)  <span class="hljs-comment">#this is now K_tilde</span><br>    np.fill_diagonal(L, val=<span class="hljs-number">0</span>)  <span class="hljs-comment">#this is now L_tilde</span><br><br>    <span class="hljs-comment">#first part in the square brackets</span><br>    trace = np.trace(np.dot(K, L))<br><br>    <span class="hljs-comment">#middle part in the square brackets</span><br>    nominator1 = np.dot(np.dot(ones.T, K), ones)<br>    nominator2 = np.dot(np.dot(ones.T, L), ones)<br>    denominator = (n - <span class="hljs-number">1</span>) * (n - <span class="hljs-number">2</span>)<br>    middle = np.dot(nominator1, nominator2) / denominator<br><br>    <span class="hljs-comment">#third part in the square brackets</span><br>    multiplier1 = <span class="hljs-number">2</span> / (n - <span class="hljs-number">2</span>)<br>    multiplier2 = np.dot(np.dot(ones.T, K), np.dot(L, ones))<br>    last = multiplier1 * multiplier2<br><br>    <span class="hljs-comment">#complete equation</span><br>    unbiased_hsic = <span class="hljs-number">1</span> / (n * (n - <span class="hljs-number">3</span>)) * (trace + middle - last)<br><br>    <span class="hljs-keyword">return</span> unbiased_hsic<br></code></pre></td></tr></table></figure>
<p>现在我们已经将HSIC的无偏估计编码实现了，可以继续处理CKA指标，CKA指标是中心核对齐算法，它对HSIC进行标准化，CKA的公式内容是由多个子模块的HSIC组成：</p>
<blockquote>
<p><em>we compute CKA as a function of average HSIC scores computed over k minibatches</em></p>
</blockquote>
<p>这里提到采取K个小批次（256）,获取A和B的激活值，计算A和B的CKA评分.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">CKA2_sample_Outorder</span>(<span class="hljs-params">X, Y</span>):</span><br>    <span class="hljs-string">&#x27;&#x27;&#x27;Computes the CKA of two matrices. This is equation (1) from the paper&#x27;&#x27;&#x27;</span><br>    n = X.shape[<span class="hljs-number">0</span>]<br>    l1 = random.sample(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, n), n)<br>    l2 = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,n)<br>    k = <span class="hljs-built_in">int</span>(n/<span class="hljs-number">256</span>)<br>    nominator = <span class="hljs-number">0</span><br>    denominator1 = <span class="hljs-number">0</span><br>    denominator2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>        index = l1[i*<span class="hljs-number">256</span>:(i+<span class="hljs-number">1</span>)*<span class="hljs-number">256</span>]<br>        X1 = X[index]<br>        Y1 = Y[index]<br>        nominator = nominator + unbiased_HSIC(np.dot(X1, X1.T), np.dot(Y1, Y1.T))<br>        denominator1 = denominator1 + unbiased_HSIC(np.dot(X1, X1.T), np.dot(X1, X1.T))<br>        denominator2 = denominator2 + unbiased_HSIC(np.dot(Y1, Y1.T), np.dot(Y1, Y1.T))<br><br>        cka = nominator / np.sqrt(denominator1 * denominator2)<br><br>    <span class="hljs-keyword">return</span> cka<br></code></pre></td></tr></table></figure>
<p>该代码做了[n/256]次采样。</p>
<p>个人代码注释：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c">cka_list.py<br><br>│  cka_paper1.py   第一篇论文的计算方法《Similarity of Neural Network Representations Revisited》<br>	输入对象是两个网络层（行数（the number of examples）相同的两个矩阵）<br>    │  cka(gram_linear(X),gram_linear(Y)) <span class="hljs-function"><span class="hljs-keyword">or</span> <span class="hljs-title">cka</span><span class="hljs-params">(gram_rbf(X),gram_rbf(Y))</span></span><br><span class="hljs-function">    |  <span class="hljs-title">feature_space_linear_cka</span><span class="hljs-params">(X,Y)</span> 和cka_linear等价</span><br><span class="hljs-function">	输入对象是两个网络 （两个列表，每个列表包括多个网络层）后面的<span class="hljs-keyword">bool</span>型True→无偏</span><br><span class="hljs-function">	|  <span class="hljs-title">cka_linear_single</span><span class="hljs-params">(net1,net2,True <span class="hljs-keyword">or</span> False)</span>对上面cka的封装</span><br><span class="hljs-function">	|  <span class="hljs-title">cka_linear_list</span><span class="hljs-params">(net1,net2,<span class="hljs-number">10</span> <span class="hljs-keyword">or</span> some number,True <span class="hljs-keyword">or</span> False)</span>数字代表要进行的切片，即分成几个小块计算求平均值，这种方法的科学性待考证</span><br><span class="hljs-function"></span><br><span class="hljs-function">│  cka_paper2.py   第二篇论文的计算方法 《WidthDeep》 全部是无偏的</span><br><span class="hljs-function">	输入对象是两个网络层（行数（the number of examples）相同的两个矩阵）</span><br><span class="hljs-function">	|  <span class="hljs-title">CKA2</span><span class="hljs-params">(X,Y)</span> 无偏HSIC的计算</span><br><span class="hljs-function">	|  <span class="hljs-title">CKA2_sample_Inorder</span><span class="hljs-params">(X,Y)</span>  按顺序抽小批次样，然后按照论文公式计算</span><br><span class="hljs-function">	|  <span class="hljs-title">CKA2_sample_Outorder</span><span class="hljs-params">(X,Y)</span> 随机放回抽小批次样，然后按照论文公式计算</span><br><span class="hljs-function">	输入对象是两个网络 （两个列表，每个列表包括多个网络层）</span><br><span class="hljs-function">	|  <span class="hljs-title">CKA2_nets</span><span class="hljs-params">(net1,net2)</span> 在函数里面可以选择，封装Inorder还是Outorder</span><br><span class="hljs-function"></span><br><span class="hljs-function">注  </span><br><span class="hljs-function">    输入对象是两个网络层 输出是一个[0,1]的浮点数</span><br><span class="hljs-function">    输入对象是两个网络   输出是一个矩阵m,每个元素都是[0,1]的浮点数（m[i,j]表示A网络第i层和B网络第j层的cka相似值）</span><br></code></pre></td></tr></table></figure>
<center><font color=red size = 6>CKA的来龙去脉Over</font></center>
<hr />
<h1 id="results"><a class="markdownIt-Anchor" href="#results"></a> Results</h1>
<blockquote>
<p>We begin our study by investigating how the depth and width of a model architecture affects its internal representation structure. How do representations evolve through the hidden layers in different architectures? How similar are different hidden layer representations to each other? To answer these questions, we use the CKA representation similarity measure outlined in Section 3.1.</p>
<p>We find that as networks become wider and/or deeper, their representations show a characteristic <em>block structure</em>: many (almost) consecutive hidden layers that have highly similar representations.</p>
</blockquote>
<p>现在开始用代码验证，首先在CIFAR-10数据集上训练ResNets.  batch大小设为256.</p>
<ul>
<li>训练三个不同深度的ResNet网络</li>
<li>用小批量数据输入到训练好的网络，保存中间结果</li>
<li>计算不同网络（或相同）各层之间的CKA</li>
<li>绘制成heatmap热图</li>
</ul>
<p>用最简单的训练方法训练10个epoch,依次训练 resnet50 | 101 | 152</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">resnet50 = create_resnet50()  <br><br>resnet50.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>resnet50.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">256</span>)<br>resnet101 = create_resnet101()<br>resnet101.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>resnet101.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">256</span>)<br>resnet152 = create_resnet152()<br>resnet152.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>,<br>              loss=tf.keras.losses.SparseCategoricalCrossentropy(),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>resnet152.fit(x_train, y_train, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">256</span>) <br></code></pre></td></tr></table></figure>
<p>用小批量数据data_batch给出中间结果intermediate_outputs_A&amp;B</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_layer_outputs_fn</span>(<span class="hljs-params">model</span>):</span><br>  <span class="hljs-string">&#x27;&#x27;&#x27;Builds and returns function that returns the output of every (intermediate) layer&#x27;&#x27;&#x27;</span><br><br>  <span class="hljs-keyword">return</span> tf.keras.backend.function([model.layers[<span class="hljs-number">0</span>].<span class="hljs-built_in">input</span>],<br>                                  [l.output <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> model.layers[<span class="hljs-number">1</span>:]])<br><span class="hljs-comment">#get function to get the output of every intermediate layer, for modelA and modelB</span><br>intermediate_outputs_A = get_all_layer_outputs_fn(modelA)(data_batch)<br>intermediate_outputs_B = get_all_layer_outputs_fn(modelB)(data_batch)<br></code></pre></td></tr></table></figure>
<p>计算resnet50和自己的CKA</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#计算两个矩阵（网络层）的cka</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_CKA_for_two_matrices</span>(<span class="hljs-params">activationA, activationB</span>):</span><br>  <span class="hljs-string">&#x27;&#x27;&#x27;Takes two activations A and B and computes the linear CKA to measure their similarity&#x27;&#x27;&#x27;</span><br><br>  <span class="hljs-comment">#unfold the activations, that is make a (n, h*w*c) representation</span><br>  shape = activationA.shape<br>  activationA = np.reshape(activationA, newshape=(shape[<span class="hljs-number">0</span>], np.prod(shape[<span class="hljs-number">1</span>:])))<br><br>  shape = activationB.shape<br>  activationB = np.reshape(activationB, newshape=(shape[<span class="hljs-number">0</span>], np.prod(shape[<span class="hljs-number">1</span>:])))<br><br>  <span class="hljs-comment">#calculate the CKA score</span><br>  cka_score = CKA(activationA, activationB)<br><br>  <span class="hljs-keyword">del</span> activationA<br>  <span class="hljs-keyword">del</span> activationB<br><br>  <span class="hljs-keyword">return</span> cka_score<br><span class="hljs-comment">#获得中间网络层的representation</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_all_layer_outputs_fn</span>(<span class="hljs-params">model</span>):</span><br>  <span class="hljs-string">&#x27;&#x27;&#x27;Builds and returns function that returns the output of every (intermediate) layer&#x27;&#x27;&#x27;</span><br><br>  <span class="hljs-keyword">return</span> tf.keras.backend.function([model.layers[<span class="hljs-number">0</span>].<span class="hljs-built_in">input</span>],<br>                                  [l.output <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> model.layers[<span class="hljs-number">1</span>:]])<br><span class="hljs-comment">#先获得所有网络层，再计算所有的cka</span><br><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compare_activations</span>(<span class="hljs-params">modelA, modelB, data_batch</span>):</span><br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">  Calculate a pairwise comparison of hidden representations and return a matrix</span><br><span class="hljs-string">  &#x27;&#x27;&#x27;</span><br> <br>  <span class="hljs-comment">#get function to get the output of every intermediate layer, for modelA and modelB</span><br>  intermediate_outputs_A = get_all_layer_outputs_fn(modelA)(data_batch)<br>  intermediate_outputs_B = get_all_layer_outputs_fn(modelB)(data_batch)<br>  <br>  result_array = np.zeros(shape=(<span class="hljs-built_in">len</span>(intermediate_outputs_A), <span class="hljs-built_in">len</span>(intermediate_outputs_B)))<br><br>  <br>  i = <span class="hljs-number">0</span><br>  <span class="hljs-keyword">for</span> outputA <span class="hljs-keyword">in</span> intermediate_outputs_A:<br>    j = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> outputB <span class="hljs-keyword">in</span> intermediate_outputs_B:<br>      cka_score = calculate_CKA_for_two_matrices(outputA, outputB)<br>      result_array[i, j] = cka_score<br>      j+=<span class="hljs-number">1</span><br>    i+= <span class="hljs-number">1</span><br><br>  <span class="hljs-keyword">return</span> result_array<br>sim = compare_activations(resnet50, resnet50, x_train[:<span class="hljs-number">256</span>])<br></code></pre></td></tr></table></figure>
<p>绘制成heatmap热图</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">15</span>), dpi=<span class="hljs-number">200</span>)<br>axes = plt.imshow(sim, cmap=<span class="hljs-string">&#x27;magma&#x27;</span>, vmin=<span class="hljs-number">0.0</span>,vmax=<span class="hljs-number">1.0</span>)<br>axes.axes.invert_yaxis() <span class="hljs-comment">#y轴反过来</span><br>plt.savefig(<span class="hljs-string">&quot;ppt.png&quot;</span>, dpi=<span class="hljs-number">400</span>)<br></code></pre></td></tr></table></figure>
<img src="D:\study\git\myblog\source\img\lab\resnet50_same.png" srcset="/img/loading.gif" lazyload alt="resnet50_same" style="zoom:8%;" />
<img src="D:\study\git\myblog\source\img\lab\resnet101_same.png" srcset="/img/loading.gif" lazyload alt="resnet101_same" style="zoom:8%;" />
<img src="D:\study\git\myblog\source\img\lab\resnet50_101.png" srcset="/img/loading.gif" lazyload alt="resnet50_101" style="zoom:8%;" />

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Tutorials/">Tutorials</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/tensorflow/">tensorflow</a>
                    
                      <a class="hover-with-bg" href="/tags/similar/">similar</a>
                    
                      <a class="hover-with-bg" href="/tags/cka/">cka</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2021/08/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E5%99%A8%E6%B1%87%E6%80%BB/">
                        <span class="hidden-mobile">梯度下降优化器汇总</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"MJAd57UreVBPxte7PDoEMye0-gzGzoHsz","appKey":"388yI2KkDdst7lb1E99J416E","placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false,"requiredFields":[]},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- LeanCloud 统计PV -->
        <span id="leancloud-site-pv-container" style="display: none">
            总访问量 
            <span id="leancloud-site-pv"></span>
             次
          </span>
      
      
        <!-- LeanCloud 统计UV -->
        <span id="leancloud-site-uv-container" style="display: none">
            总访客数 
            <span id="leancloud-site-uv"></span>
             人
          </span>
      

    
  </div>


  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.3/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.1/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


  <script defer src="/js/leancloud.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?e6b05da17eecff6523584dd51e7da014";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
